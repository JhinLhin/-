# Key Technologies

## Redis

想象 Redis 是一个**超快的仓库管理员**，他把所有东西放在**桌子上**（内存），不放仓库（硬盘）。你要什么他秒给你，但断电了桌上的东西就没了。

------

**基础概念**

**内存 vs 硬盘**：普通数据库把数据存硬盘，读写慢但安全。Redis 存内存，极快但断电丢数据。这是刻意的取舍。

**单线程**：Redis 同一时间只做一件事，不会两个操作互相干扰，所以不用担心并发冲突。听起来慢，但因为在内存里操作，实际快得离谱——每秒能处理 **10万次写入**，读取延迟**微秒级**（比毫秒还快1000倍）。

**键值对**：Redis 里所有数据都是 `key → value` 的形式。key 是字符串，value 可以是各种类型。就像字典：你用一个词（key）查它的释义（value）。

------

**数据结构白话解释**

**String**：最简单，存一个值。比如 `用户123的登录次数 → 5`

**Hash**：存一个对象的多个字段。比如一个用户：`{名字: 张三, 年龄: 25, 城市: 北京}`。相当于一行数据库记录。

**List**：有顺序的列表，可以从头尾插入。像排队。

**Set**：无序集合，自动去重。像一个装球的桶，同一个球只能装一个。

**Sorted Set（有序集合）**：每个元素带一个分数，自动按分数排序。像游戏排行榜：每个玩家有积分，Redis 帮你自动排好序。

**Bloom Filter**：用来快速判断"这个东西**可能**存在 / **肯定不**存在"。会有误判（说存在但其实不在），但绝不会漏判（说不存在就一定不在）。用于快速过滤，省得每次都去数据库查。

**Geo（地理位置）**：存经纬度，支持"查找附近XX公里内的东西"。

**Stream（流）**：一个只能往后追加的日志，像传送带，生产者往上放消息，消费者从上取消息处理。

------

**六大用途，每个用大白话讲清楚**

**1. 缓存（最常用）**

**场景**：电商网站，商品详情页每秒被访问几万次。每次都去数据库查？数据库会崩。

**做法**：第一次查数据库，把结果存到 Redis（`product:123 → {...商品信息...}`）。后续请求直接从 Redis 拿，不碰数据库。

**TTL（过期时间）**：给每个缓存设个有效期，比如10分钟后自动删掉。这样数据不会永远占内存，也能定期刷新。

**Hot Key 问题**：假设你有100台 Redis 服务器，数据均匀分布。突然某个商品爆火，所有请求都打到存这个商品的那1台服务器，它就崩了。

解法：

- 在每台应用服务器本地也缓存一份（本地内存），减少打到 Redis 的次数
- 把同一份数据存到多个 key，请求随机打到不同 key（不同服务器）
- 给热点 key 的服务器加副本，多台机器分担读请求

------

**2. 分布式锁**

**场景**：演唱会抢票，最后1张票，1000个人同时点"购买"。怎么保证只有1个人买到？

**问题**：如果1000台服务器同时去数据库检查"还有没有票"，都看到有1张，然后都下单，就超卖了。

**做法**：用 Redis 的 `INCR`（原子自增）。所有服务器都去对同一个 key 执行 `INCR`：

- 返回 `1`：我是第一个，我拿到锁，我去处理购票
- 返回 `>1`：别人先拿到了，我等一下再试

为什么 `INCR` 可以？因为 Redis 单线程，`INCR` 是原子操作，不可能两个人同时拿到 `1`。

完成后 `DEL` 这个 key，锁释放，别人可以继续抢下一次。

- 应该就是-1

------

**3. 排行榜**

**场景**：游戏积分榜、热搜榜、点赞最多的帖子。

**做法**：用 Sorted Set。`ZADD 榜单名 分数 用户ID`，Redis 自动帮你排序。

查 Top10？一个命令。某用户排第几？一个命令。比你自己写 SQL 排序快多了，而且实时更新。

------

**4. 限流（Rate Limiting）**

**场景**：API 接口，每个用户每分钟最多调用100次，防止有人恶意刷接口。

**做法**：

- 用户每来一个请求，对 `用户ID:当前分钟` 这个 key 执行 `INCR`
- 如果计数 ≤ 100，放行
- 如果 > 100，拒绝
- 设置 `EXPIRE` 60秒，下一分钟计数自动清零

------

**5. 附近的人 / 地理搜索**

**场景**：Uber 查找附近3公里内的司机，外卖查附近餐厅。

**做法**：司机上线时 `GEOADD` 把经纬度存进去，用户打车时 `GEOSEARCH` 查附近，Redis 帮你算距离和排序。

底层用 **geohash**：把地球切成一个个小格子，先找候选格子里的点，再精确计算距离过滤。

------

**6. 消息系统**

**Pub/Sub（发布/订阅）—— 实时广播**

**比喻**：微信群。你发一条消息，所有在群里的人都收到。但你发消息时不在线的人，永远看不到这条消息。

**适合**：聊天系统、实时通知、直播弹幕。

**致命缺点**：消息不存储，离线就丢。

**连接方式**：每个客户端连接到集群的每个节点（连接数 = 节点数），而不是每个频道一个连接，所以百万频道也不会有百万连接。

**Streams（流）—— 可靠消息队列**

**比喻**：传送带 + 任务单。每个消息放上传送带，工人取走处理，处理完打勾确认。如果工人中途挂了，任务单还在，另一个工人可以接着做。

**适合**：需要保证消息不丢的场景，比如订单处理、日志收集。

**对比 Pub/Sub**：

|              | Pub/Sub  | Streams      |
| ------------ | -------- | ------------ |
| 消息持久化   | ❌ 丢     | ✅ 存着       |
| 离线能收到吗 | ❌        | ✅            |
| 适合场景     | 实时通知 | 可靠任务队列 |

------

**集群怎么工作**

Redis 可以跑多台机器（集群）。数据怎么分配？用 **hash slot**：

把所有 key 的空间分成 16384 个槽，每台机器负责一部分槽。客户端本地存着一张"槽→机器"的地图，直接连对应的机器拿数据，不用中转。

**关键限制**：一次请求涉及的数据必须在同一台机器上。所以 key 怎么设计很重要——相关联的数据要用能落到同一节点的 key。

**A. Single-Node（单机）**

- 只有一个 **Main（主节点）**

> 面试一句话：**单机简单，但不 HA、不好水平扩容。**

**B. Replicated（主从/副本，HA）**

- 一个 **Main** + 一个 **Secondary（Replica）**
- **Main 写入**后把数据**复制**到 Secondary（通常是异步）
- **解决的问题**：**高可用（HA）**
  - Main 挂了可以切到 Secondary（failover）

> 面试一句话：**副本 = 为了 HA（以及读扩展），不是为了解决容量上限。**

**C. Cluster（集群 = 分片 + 每片副本）**

你可以把 **整个 Redis Cluster 的 key 空间**切成很多个“槽”（slot）。

- **真实 Redis Cluster 是 16384 个 slots**

- 每个 **Main 节点负责一部分 slots**

  - 一个 Main 负责一段 key 范围

    每个 Main 旁边有对应 Secondary（副本）

- 一个 key 属于哪个 slot 是固定算法算出来的（客户端能算）

**Cluster 解决的问题**：

1. **容量扩展**：数据分到多台 Main 上（分片）
2. **高可用**：每片还有副本

**为什么设计成 slot 分片而不是 hash(key)%机器数？**

因为机器数量会变。

如果直接：

```
node = hash(key) % 机器数
```

当机器从 3 台变 4 台：

👉 所有 key 位置都会变
 👉 全部迁移
 👉 系统崩溃级开销

------

而 Redis 的方式：

```
key → slot（固定）
slot → node（可变）
```

只需要迁移部分 slot 就能扩容。

> 面试一句话：**Cluster = 水平扩容 + HA（每个分片一主多从）。**

------

**1) Redis Streams：像 Kafka topic 的“可追溯日志”（append-only log）**

**它解决的核心问题是什么？**

你想要一种机制：

- **把事件“可靠地写进一个日志”**（append-only：只追加，不覆盖）
- 然后让**多个 worker 分布式地消费**这些事件
- **worker 挂了也不能丢**，要能让别人接手继续处理

这就是你文中说的“Event Sourcing / Work Queue”的核心。

------

**Streams 的基本用法（工作队列视角）**

你可以把 Stream 当成一个队列的“事件列表”：

- **生产者**：把任务/事件追加进去（`XADD`）
- **消费者**：worker 们从里面拿任务做（通过 consumer group 用 `XREADGROUP`）
- **失败恢复**：某 worker 处理到一半挂了，其他 worker 可以把它“认领过来”重做（`XCLAIM`）

**为什么 consumer group 很关键？**

因为它会“记账”：

- 哪些消息已经被哪个 worker 领走了
- 哪些还没确认完成（你可以理解成“待处理列表”）

所以当 worker 挂了，就能知道它手里还有哪些没做完 → 让别人接手。

**2) Redis Pub/Sub：实时广播（但不保证你一定收到）**

想象你订阅了一个**微信公众号**：

- **公众号主（Publisher）** = 发消息的人
- **公众号（Channel）** = 消息的频道
- **订阅者（Subscriber）** = 收消息的人

当公众号主发一条消息 → **所有订阅者同时收到**，这就是 Pub/Sub！

------

**Redis Pub/Sub 怎么工作？**

```
1. 第一步：发布者发送消息 (Publisher -> Redis)
动作：发布者（比如一个后端 API 服务）和 Redis 集群中的某个节点建立连接。

命令：它向 Redis 发送一条指令，比如 SPUBLISH room_888 "大家好！"。（这里用 S 开头代表我们在用文档里提到的最新版的 Sharded 分片发布）。

细节：发布者发完这条消息后，它的任务就结束了，不需要等待任何人已读回复。

2. 第二步：Redis 集群内部路由 (Inside Redis Cluster)
动作：Redis 节点收到这条消息后，开始进行内部处理。

传统模式 vs 新版分片模式：

以前老版本的 Redis 会用“大喇叭”把这条消息广播给集群里的所有节点，这很浪费服务器资源。

现在（也就是你文档里强调的 Sharded Pub/Sub），Redis 变聪明了。它通过算法算出 room_888 这个频道归哪个特定的节点管。消息直接交给那个负责的节点，没有多余的内部废话和网络开销，这也是为什么现在的 Redis Pub/Sub 能支撑大规模并发的原因。

3. 第三步：Redis 推送给客户端 (Redis -> Subscribed Clients)
动作：负责 room_888 的那个 Redis 节点，会在自己的内存里查一个“订阅字典”。

推送：它发现 Client A 和 Client B 之前已经订阅了这个频道。于是，Redis 顺着之前已经建立好且一直保持开启的那条 TCP 长连接（就是我们在上一张图里看到的那条唯一的粗管道），直接把 "大家好！" 这条消息 推 (Push) 过去。

细节：注意，这里是纯纯的推送（Push），客户端不需要一直去问 Redis “有我的新消息吗？（Pull）”。只要连接连着，数据就会像水流一样源源不断地流向客户端。
```

两个核心命令：

- `SPUBLISH channel message` → 发消息
- `SSUBSCRIBE channel` → 订阅频道

------

**最重要的特性：At Most Once（最多一次）**

> 如果你**订阅者下线**了，消息发来的时候你**永远收不到**了！

就像直播间——你没在线，主播说的话你就错过了，**没有回放**。

------

**什么时候用？什么时候不用？**

| 场景           | 用 Pub/Sub？ | 原因         |
| -------------- | ------------ | ------------ |
| 聊天室实时消息 | ✅ 用         | 快、简单     |
| 实时通知       | ✅ 用         | 无需持久化   |
| 订单支付确认   | ❌ 不用       | 必须保证送达 |
| 离线用户消息   | ❌ 不用       | 会丢消息     |

❌ 需要持久化 → 用 **Redis Streams** 或 **Kafka**

------

**为什么不要自己造轮子？**

有人想自己实现 Pub/Sub（用 Redis 存 key-value 记录谁在哪个频道），但问题是：

- 原生 Pub/Sub：**2次网络跳转**（客户端→Redis→订阅者）
- 自制版本：**3次网络跳转**（客户端→Redis查表→Redis返回→再连接每个服务器）

更多跳转 = 更慢 + 更复杂。**别自己造轮子，直接用原生的！**

------

🎤 面试必备总结

> "Redis Pub/Sub 是一个实时消息广播机制，支持一对多消息传递，适合聊天、实时通知等场景。"

核心特性（必背）

- **实时性**：消息即发即达，延迟极低
- **At Most Once**：最多送达一次，**不保证送达**，不持久化
- **连接模型**：每个节点一个连接，不是每个 channel 一个连接（可扩展）
- **Sharded Pub/Sub**：新版 Redis 支持分片，解决了旧版扩展性问题

## Elasticsearch

> ## 第一步：先搞懂 Elasticsearch 是什么
>
> ### 一句话理解
>
> > Elasticsearch = **超强搜索引擎**，就像给你的数据装了一个 Google
>
> 普通数据库（Postgres）：**"帮我找价格低于100的书"** → ✅ 没问题
>  Elasticsearch：**"帮我找所有提到'爱情'但不提'悲剧'、评分高于4星、按相关性排序的书"** → ✅ 轻松搞定
>
> ------
>
> ## 第二步：四个核心概念（必须先懂）
>
> 用书店来理解：
>
> ```
> 📚 书店类比
> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
> Document（文档）  = 一本书（一个JSON对象）
> Index（索引）     = 一个书架（一类文档的集合）  
> Mapping（映射）   = 书架的分类规则（哪些字段可搜索）
> Field（字段）     = 书的属性（标题、作者、价格...）
> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
> ```
>
> ### Document 长这样
>
> ```json
> {
>   "id": "XYZ123",
>   "title": "The Great Gatsby",
>   "author": "F. Scott Fitzgerald",
>   "price": 10.99
> }
> ```
>
> 这就是一个 Document，本质是 **JSON对象**，不一定是"文档"，可以是任何东西（用户、订单、商品...）
>
> ### Mapping 的关键作用
>
> Mapping 是属于整个 index 的，不是每个 document 一个。
>
> Mapping 决定每个字段怎么被处理：
>
> | 字段类型        | 底层存储方式     | 适合做什么                  |
> | --------------- | ---------------- | --------------------------- |
> | `keyword`       | 完整值（不分词） | 精确匹配、排序 → 想成哈希表 |
> | `text`          | 分词后倒排索引   | 全文搜索 → 想成倒排索引     |
> | `float/integer` | 数字             | 范围查询、排序              |
> | `date`          | 时间戳           | 时间范围查询                |
> | `nested`        | 嵌套文档         | 搜索对象数组内部            |
>
> **💡 重要：** Mapping里没有的字段 = 不可搜索！多余的字段 = 浪费内存！
>
> **一个 index 里的 document 格式必须一样吗？**
>
> 答案是：
>
> > ❌ **不必须完全一样**
> >
> > ✔ 但字段类型必须兼容 mapping
>
> ------
>
> ✔ 允许不同字段
>
> 合法：
>
> ```
> doc1:
> { "name": "iphone", "price": 999 }
> 
> doc2:
> { "name": "ipad" }
> ```
>
> 因为：
>
> - price 只是缺失，不冲突
>
> ------
>
> ❌ 不允许字段类型冲突
>
> 如果 mapping 里：
>
> ```
> price → float
> ```
>
> 你写入：
>
> ```
> { "price": "cheap" }
> ```
>
> 就会报错：
>
> ```
> mapper_parsing_exception
> ```
>
> 因为：
>
> > ES 一旦确定字段类型，就不能变
>
> ## 第三步：基本操作（CRUD）
>
> ### 创建 Index
>
> ```
> PUT /books  →  创建一个叫"books"的书架
> ```
>
> ### 设置 Mapping（决定哪些字段可搜索）
>
> ```
> PUT /books/_mapping  →  告诉ES这个书架怎么分类
> ```
>
> ### 添加 Document
>
> ```
> POST /books/_doc  →  把一本书放上书架（ES自动生成ID）
> ```
>
> ### 更新 Document（⚠️ 有坑）
>
> ```
> 方法1: PUT整个文档  →  覆盖整本书（危险：并发时会覆盖别人的修改）
> 方法2: PUT + ?version=1  →  只有版本匹配才更新（乐观锁，安全）
> 方法3: POST /_update  →  只更新某个字段（最常用）
> ```
>
> **乐观并发控制 Optimistic Concurrency Control：**
>
> ```
> 我有版本1  →  我改价格  →  同时你也改了价格（你的是版本2了）
> 我提交时说"只在版本=1时更新"  →  ES发现已经是版本2了  →  报错！
> 我收到报错  →  重新拉取最新版本再改  →  安全！
> ```
>
> ------
>
> ## 第四步：搜索（最重要！）
>
> ### 搜索长这样
>
> ```
> 简单搜索：找title里有"Great"的书
> GET /books/_search
> { "query": { "match": { "title": "Great" } } }
> 复合搜索：找title有"Great"且price≤15的书
> GET /books/_search
> {
>   "query": {
>     "bool": {
>       "must": [
>         { "match": { "title": "Great" } },
>         { "range": { "price": { "lte": 15 } } }
>       ]
>     }
>   }
> }
> ```
>
> ### 搜索结果含分数！
>
> ```json
> "hits": [
>   { "_score": 2.18, "_source": { "title": "The Great Gatsby" } },
>   { "_score": 1.98, "_source": { "title": "Great Expectations" } }
> ]
> ```
>
> `_score` = **相关性分数**，基于 TF-IDF 算法（越相关越高）
>
> ------
>
> ## 第五步：排序与翻页
>
> ### 排序
>
> ```
> 按价格升序  →  "sort": [{ "price": "asc" }]
> 多字段排序  →  先按价格升序，再按日期降序
> 脚本排序    →  用自定义计算公式排序（打折后价格等）
> ```
>
> 不指定排序时默认按 `_score` 相关性排序。
>
> ### 翻页（3种方式，理解差异！）
>
> ```
>                 from/size          search_after        游标(PIT)
> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
> 原理           指定起始位置        用上页最后一条        快照+search_after
> 性能           深翻页很慢⚠️         高效✅               高效✅  
> 数据一致性      可能重复/跳过        可能漂移              完全一致✅
> 能否跳页        ✅                   ❌只能前进            ❌只能前进
> 适合场景        前几页结果           无限滚动               精确分页
> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
> ```
>
> **from/size 为什么深翻页慢？**
>
> > 你要第10000页的10条 → ES必须先找出100010条，排序，再取最后10条 → 超级浪费！
>
> ------
>
> ## 第六步：底层原理（面试加分区）
>
> ### 大架构：ES 是 Lucene 的"外壳"
>
> ```
> Elasticsearch（你与它交互）
>     └── 负责：集群管理、分布式、API、实时性
>          └── Apache Lucene（真正干搜索的核心）
>               └── 负责：倒排索引、数据结构、搜索算法
> ```
>
> ### 集群节点类型
>
> Elasticsearch 本质上是一个**[分布式数据库](https://zhida.zhihu.com/search?content_id=236633439&content_type=Article&match_order=1&q=分布式数据库&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NzIwNDAwNTYsInEiOiLliIbluIPlvI_mlbDmja7lupMiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoyMzY2MzM0MzksImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.Q4VNwmRWlVQ6j6A84C3aj-HcxfYl47bEF_TvOTatyHk&zhida_source=entity)**，允许多台服务器协同工作，每台服务器可以运行多个 Elasticsearch 实例。单个 Elasticsearch 实例称为一个节点（Node），一组节点构成一个集群（Cluster）。
>
> ```
> 你的请求
>    ↓
> [Coordinating Node] ← 接收请求、分发任务、汇总结果（"前台经理"）
>    ↓                ↗
> [Data Node 1]  [Data Node 2]  [Data Node 3]  ← 存数据、执行搜索（"仓库工人"）
>    ↑
> [Ingest Node] ← 数据进来前先处理（"预处理员"）
> [Master Node] ← 管理集群（"CEO"）
> ```
>
> ### 数据节点：俄罗斯套娃结构
>
> ```
> Elasticsearch Index（索引）
>   └── Shard（分片）× N   ← 横向扩展，分布在不同节点
>        └── Replica（副本） ← 高可用 + 提升读性能
>             └── Lucene Index（Lucene索引）
>                  └── Segment（段）× 多个   ← ⭐ 核心单元！
> ```
>
> **Shard 的意义：** 把1亿条数据分成10个 Shard，每个 Shard 只存1000万条 → 10个节点并行搜索 → 快10倍！
>
> **Replica 的意义：**
>
> - 高可用：一个节点挂了，还有备份
> - 提升读性能：1个 Shard 能处理X个查询，3个 Replica → 处理3X个查询
>
> ------
>
> ## ⭐ 最关键：Lucene Segment 的设计天才
>
> ```
> Index
>  ├── Shard0
>  │     ├── seg_1
>  │     ├── seg_2
>  │     └── seg_3
>  └── Shard1
>        ├── seg_1
>        └── seg_2
> ```
>
> ### Segment 是不可变的（Immutable）！
>
> 这是整个系统最精妙的设计，理解它你就懂了90%的底层原理。
>
> ```
> 写入流程
> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
> 新文档进来 → 先放内存缓冲区
> 批量积累后 → 构建新 Segment → 写入磁盘
> 多个小Segment → 定期合并成大Segment（清理被删文档）
> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
> ```
>
> **删除怎么处理？**
>
> > Segment 不可修改，所以"删除"只是在一个**删除标记列表**里记"这个ID删了"
> >  真实删除发生在 **Segment 合并** 的时候
>
> **更新怎么处理？**
>
> > = 软删除旧文档 + 插入新文档
> >  ⚠️ 所以频繁更新的数据不适合 ES！
>
> ### 为什么不可变性是天才设计？
>
> | 好处       | 原因                                   |
> | ---------- | -------------------------------------- |
> | 高效缓存   | 数据不变，安全缓存，不用担心数据不一致 |
> | 并发简单   | 读不需要加锁（反正不变）               |
> | 压缩效果好 | 静态数据压缩率更高                     |
> | 崩溃恢复快 | 状态确定，恢复简单                     |
>
> ------
>
> ## ⭐ Segment 里的两个神器
>
> ### 神器1：倒排索引（Inverted Index）
>
> **问题：** 10亿本书，找所有标题含"lazy"的书
>
> **没有倒排索引：** 扫描10亿本书 → O(n)，超慢
>
> **倒排索引：**
>
> ```
> "lazy"   → [文档12, 文档53, 文档891, ...]
> "great"  → [文档1, 文档7, 文档34, ...]
> "gatsby" → [文档1, 文档203, ...]
> ```
>
> > 提前建好"词 → 文档列表"的映射
> >  搜"lazy" → 直接查这张表 → O(1)，超快！
>
> **类比：** 就是书后面的**索引页**！你找"光合作用"不会从第一页翻，直接查索引页找页码。
>
> ### 神器2：Doc Values（列式存储）
>
> **问题：** 找到了1000本相关书，要按价格排序
>
> ```
> 传统行存储（普通数据库）：
> [书1: title="Great Gatsby", author="Fitzgerald", price=9.99, ...]
> [书2: title="Mockingbird",  author="Lee",        price=12.99, ...]
> 要排价格 → 读每一整行 → 慢（读了大量无用数据）
> 
> Doc Values（列存储）：
> price列: [9.99, 12.99, 10.50, 7.99, ...]  ← 连续内存
> 排序时 → 只读这一列 → 快！
> ```
>
> **一句话：** 倒排索引找到"哪些文档匹配"，Doc Values 负责"排序和聚合"
>
> ------
>
> ## 查询执行流程可视化
>
> ```
> 用户搜索："Great" 且 price < 15
> 
>          ┌─────────────────────────────────────────┐
>          │          Coordinating Node               │
>          │  1. 解析查询                              │
>          │  2. Query Planning（优化执行顺序）         │
>          │  3. 分发到所有相关Shard                   │
>          └──────────┬──────────────────┬───────────┘
>                     ↓                  ↓
>           ┌──────────────┐    ┌──────────────┐
>           │  Data Node 1 │    │  Data Node 2 │
>           │  Shard 1     │    │  Shard 2     │
>           │              │    │              │
>           │ Phase 1:查询  │    │ Phase 1:查询  │
>           │ 倒排索引找ID  │    │ 倒排索引找ID  │
>           │              │    │              │
>           │ Phase 2:取数  │    │ Phase 2:取数  │
>           │ 用_source取文档│    │ 用_source取文档│
>           └──────┬───────┘    └──────┬───────┘
>                  └────────┬──────────┘
>                           ↓
>                  Coordinating Node
>                  汇总、排序、返回结果
> ```
>
> ------
>
> ## 面试使用 Elasticsearch 的注意事项
>
> ### ✅ 什么时候用
>
> - 复杂全文搜索（电商搜索、内容搜索）
> - 需要相关性排序
> - 多条件过滤 + 排序 + 聚合
> - 日志分析
>
> ### ❌ 什么时候不用
>
> - 数据量小（<10万条）→ 普通DB就够了
> - 写入非常频繁 → ES不擅长高频更新
> - 需要强一致性 → ES是最终一致性
> - 当主数据库 → ES不是数据库！
>
> ### 💡 标准用法（面试必说）
>
> ```
> 用户写入 → Postgres（主数据库）
>               ↓ CDC（Change Data Capture，变更捕获）
>            Elasticsearch（搜索引擎）
>               ↑
>           用户搜索
> ```
>
> > **两个数据库同步，Postgres 是权威，ES 是搜索专用副本**
>
> ------
>
> # 🎤 面试必备终极总结
>
> ## 一句话介绍
>
> > "Elasticsearch 是基于 Lucene 的分布式搜索引擎，通过倒排索引实现快速全文搜索，通过分片实现横向扩展，适合复杂搜索、排序、聚合场景。"
>
> ## 架构关键词
>
> **Lucene + Segment（不可变）+ 倒排索引 + Doc Values + Shard（分片）+ Replica（副本）+ Coordinating Node（协调节点）**
>
> ## 面试必答框架
>
> **面试官："如何为电商设计搜索功能？"**
>
> > "商品数据先写入 Postgres（权威存储），通过 CDC 同步到 Elasticsearch。搜索时用 ES 的 bool query 组合全文匹配 + 价格范围过滤 + 品牌精确匹配。结果按相关性分数排序，翻页用 search_after 避免深翻页性能问题。
> >
> > 需要注意 ES 是最终一致性，写入后有短暂延迟才可搜索，且不适合频繁更新的字段。"
>
> ## 底层原理对话（加分）
>
> **面试官："为什么 ES 搜索快？"**
>
> > "两个核心原因：第一，倒排索引把'词→文档'的映射预先建好，搜索时 O(1) 查找而不是 O(n) 扫描；第二，分片让搜索在多个节点并行执行，结果由协调节点汇总，线性扩展性能。"
>
> **面试官："ES 的 Segment 为什么设计成不可变的？"**
>
> > "不可变带来三个好处：可以安全缓存（数据不变不用担心一致性）；读操作不需要锁（并发简单）；压缩效率更高。代价是删除和更新需要软标记 + 定期 Merge，所以 ES 不适合高频更新场景。"
>
> ## 对比记忆
>
> |          | Elasticsearch      | Postgres全文索引 | Kafka    |
> | -------- | ------------------ | ---------------- | -------- |
> | 适合     | 复杂搜索+排序+聚合 | 简单全文搜索     | 消息队列 |
> | 数据量   | 大规模             | 中小规模         | 大规模   |
> | 一致性   | 最终一致           | 强一致           | 最终一致 |
> | 更新性能 | 差（不可变架构）   | 好               | -        |
> | 搜索能力 | 极强               | 有限             | 无       |
>
> ## 面试雷区（说出来秒杀）
>
> 1. **ES 不是主数据库** → 配合 Postgres/DynamoDB + CDC 使用
> 2. **ES 最终一致性** → 写入后有延迟，主动提出这个限制
> 3. **高频更新场景慎用** → Segment 不可变，更新=软删除+插入，代价高
> 4. **深翻页用 search_after** → from/size 在深页面性能极差
> 5. **Mapping 精简** → 只索引需要搜索的字段，否则浪费内存

## Kafka

> ## 一、核心比喻：世界杯赛事系统
>
> 理解 Kafka，从一个故事开始：
>
> ```
> 世界杯发生进球事件
>        ↓
> 放入队列（Kafka Topic）
>        ↓
> 消费者读取事件 → 更新网站
> ```
>
> **问题随着规模扩大而出现：**
>
> - 1000支球队同时比赛 → 单台服务器撑不住
> - 多个消费者同时读 → 同一事件被重复处理
> - 足球和篮球事件混在一起 → 消费者不知道该读哪个
>
> Kafka 的三个核心设计就是为了解决这三个问题：**Partition（分区）、Consumer Group（消费者组）、Topic（主题）**
>
> ------
>
> ## 二、Kafka 是什么？
>
> > **一句话定义**：Kafka 是一个**分布式事件流平台**，既能当消息队列用，也能当实时数据流处理系统用。
>
> **两种使用模式对比：**
>
> | 模式         | 特点                                 | 适用场景               |
> | ------------ | ------------------------------------ | ---------------------- |
> | **消息队列** | 每条消息只被一个消费者处理           | 异步任务、解耦服务     |
> | **事件流**   | 数据可被多个消费者组独立读取，可回放 | 实时数据处理、日志分析 |
>
> ------
>
> ## 三、核心术语（必须掌握）
>
> ### 架构层次图
>
> ```
> Kafka 集群（Cluster）
>     │
>     ├── Broker（服务器）① ── Broker（服务器）② ── Broker（服务器）③
>     │        │
>     │    Partition 0    ← 属于 Topic A
>     │    Partition 1    ← 属于 Topic A
>     │    Partition 2    ← 属于 Topic B
>     │
> Producer（生产者）→ 写入数据
> Consumer（消费者）← 读取数据
> ```
>
> ### 各术语详解
>
> **Broker（经纪人/服务器）**
>
> - 就是一台服务器，负责存储数据和服务客户端
> - `多个 Broker 组成 Kafka 集群`
>
> **Topic（主题）**
>
> - 逻辑上的数据分类，类比"频道"
> - 足球事件放 `soccer-topic`，篮球事件放 `basketball-topic`
> - `消费者订阅自己感兴趣的 Topic`
>
> **Partition（分区）**
>
> - Topic 的物理实现，是一个**只能追加（append-only）的日志文件**
> - 每个 `Topic 可以有多个 Partition`，分布在`不同 Broker 上`
> - **Partition 是 Kafka 实现并行和扩展的核心**
> - 这里每个partition里的事件就是一个消息
>
> ```
> Topic "soccer-events"
> ├── Partition 0: [事件A, 事件D, 事件G, ...]  ← Broker 1
> ├── Partition 1: [事件B, 事件E, 事件H, ...]  ← Broker 2
> └── Partition 2: [事件C, 事件F, 事件I, ...]  ← Broker 3
> ```
>
> **Topic vs Partition 区别**：
>
> - Topic = 逻辑分组（"篮球"频道）
> - Partition = 物理存储（实际的日志文件）
>
> **Producer（生产者）**：写数据到 Topic 的服务
>
> **Consumer（消费者）**：从 Topic 读数据的服务
>
> **Consumer Group（消费者组）**：
>
> - 多个消费者组成一个组
> - **每个 Partition 只分配给组内一个消费者**
> - 保证每条消息只被处理一次
>
> 可理解为每个topic一个consumer group，然后一个group里的每个consumer处理一个该topic的partition
>
> ```
> Consumer Group "soccer-updaters"
> ├── Consumer A → 负责 Partition 0
> ├── Consumer B → 负责 Partition 1
> └── Consumer C → 负责 Partition 2
> ```
>
> **Offset（偏移量）**：
>
> ![image-20260222130414304](C:\Learning Notes\Java\系统设计之神\img\image-20260222130414304.png)
>
> - 每条消息在 Partition 中的唯一编号（0, 1, 2, 3...）
> - 消费者通过 Offset 记录"我读到哪了"
> - 消费者崩溃重启后，从上次提交的 Offset 继续读
>
> ------
>
> ## 四、消息的完整生命周期
>
> ### 一条消息的结构
>
> ```
> {
>   key:       "game_123"        ← 决定进哪个 Partition
>   value:     "Goal scored!"   ← 实际内容（payload）
>   timestamp: 1704067200000    ← 创建时间
>   headers:   { source: "referee_system" }  ← 元数据
> }
> ```
>
> ### 消息从生产到消费的全流程
>
> ```
> ① Producer 创建消息，带上 key
> 
> ② Kafka 计算目标 Partition：
>    partition = hash(key) % 分区总数
>    （相同 key → 相同 Partition → 保证顺序）
> 
> ③ 消息写入该 Partition 的 Leader 副本
> 
> ④ Leader 将消息同步给 Follower 副本（复制机制）
> 
> ⑤ Producer 收到确认（ack）
> 
> ⑥ Consumer 主动 poll（拉取）新消息
> 
> ⑦ Consumer 处理消息后，提交 Offset
> ```
>
> ### 复制机制（Replication）保证数据安全
>
> ```
> Partition 0 的副本分布：
> ├── Broker 1: Leader（处理读写请求）
> ├── Broker 2: Follower（被动同步数据）
> └── Broker 3: Follower（被动同步数据）
> 
> 如果 Broker 1 挂掉 → Kafka 自动选 Broker 2 或 3 成为新 Leader
> ```
>
> - **Replication Factor = 3**（最常用）：1个Leader + 2个Follower
> - **acks = all**：消息写入所有同步副本后才算成功，最强持久性保证
>
> ![image-20260222130546954](C:\Learning Notes\Java\系统设计之神\img\image-20260222130546954.png)
>
> ------
>
> ## 五、面试：什么时候用 Kafka？
>
> ### ✅ 用作消息队列（异步处理）
>
> **场景特征**：有些工作不需要立刻做，可以排队慢慢处理
>
> | 面试例子              | Kafka 的用途                                          |
> | --------------------- | ----------------------------------------------------- |
> | **YouTube 视频上传**  | 上传后立即可看标清，把视频S3链接放Kafka，后台慢慢转码 |
> | **Ticketmaster 购票** | 用 Kafka 实现虚拟排队，保证先来先得的顺序             |
> | **微服务解耦**        | 服务A生产消息，服务B慢慢消费，互不影响                |
>
> ### ✅ 用作事件流（实时处理）
>
> **场景特征**：需要持续、实时处理数据，或同一数据被多方使用
>
> | 面试例子         | Kafka 的用途                           |
> | ---------------- | -------------------------------------- |
> | **广告点击聚合** | 实时统计每个广告的点击数               |
> | **FB Live 评论** | 评论作为事件流，同时推送给多个下游服务 |
> | **实时监控**     | 系统日志实时分析                       |
>
> ### 判断决策树
>
> ```
> 需要异步处理？
>     ├── 是 → 消息队列模式
>     │         ├── 需要内置重试和死信队列 → 用 AWS SQS
>     │         └── 需要高吞吐/消息保留/顺序保证 → 用 Kafka
>     │
>     └── 需要实时流处理或多方消费同一数据 → Kafka 流模式
> ```
>
> ------
>
> ## 六、扩展性（Scalability）
>
> ### 单个 Broker 的容量估算
>
> - 存储：约 **1TB** 数据
> - 吞吐：约 **100万条消息/秒**
> - 消息大小建议：**< 1MB**（大文件存 S3，Kafka 只存指针）
>
> ### 如何扩展
>
> **方法1：增加 Broker**
>
> - 直接加服务器，但要确保 Partition 数量足够，否则新 Broker 没活干
>
> **方法2：增加 Partition（面试重点！）**
>
> - 更多 Partition = 更多并行 = 更高吞吐
>
> **最关键决策：选择正确的 Partition Key**
>
> ```
> 好的 Key：均匀分布数据
>   ├── 用户ID → 每个用户的事件有序
>   ├── 地区ID → 按地区分区
>   └── 游戏ID → 每场比赛的事件有序
> 
> 坏的 Key：导致热分区（Hot Partition）
>   └── 广告ID → Nike新广告瞬间大量点击 → 单个分区过载！
> ```
>
> ### 热分区（Hot Partition）解决方案
>
> > 面试官超爱问这个！
>
> | 方案         | 做法                          | 代价             |
> | ------------ | ----------------------------- | ---------------- |
> | **不设 key** | 让 Kafka 随机分配             | 失去顺序保证     |
> | **随机加盐** | key = `ad_id + random_suffix` | 消费者聚合变复杂 |
> | **复合 key** | key = `ad_id + region`        | 设计复杂         |
> | **背压控制** | 消费者积压时减慢生产者速度    | 增加延迟         |
>
> ------
>
> ## 七、容错性（Fault Tolerance）
>
> ### 消费者宕机怎么办？
>
> ```
> Offset 机制 = 书签
>               ↓
> 消费者读消息 → 处理完 → 提交 Offset
> 
> 重启后：
>   从上次提交的 Offset 继续读
>   （可能重复处理最后几条消息 = At-Least-Once）
> ```
>
> ### 精确一次 vs 至少一次
>
> | 语义                          | 默认？       | 场景                     |
> | ----------------------------- | ------------ | ------------------------ |
> | **At-Least-Once（至少一次）** | ✅ 默认       | 消费者崩溃后可能重复处理 |
> | **Exactly-Once（精确一次）**  | ❌ 需额外配置 | 幂等Producer + 事务API   |
>
> ### Consumer Group 的自动故障转移
>
> ```
> 正常：Consumer A → Partition 0
>       Consumer B → Partition 1
>       Consumer C → Partition 2
> 
> Consumer C 挂掉后，Kafka 自动 Rebalance：
>       Consumer A → Partition 0 + Partition 2
>       Consumer B → Partition 1
> ```
>
> ------
>
> ## 八、消费者重试 & 死信队列（DLQ）
>
> Kafka 原生**不支持**消费者重试，需要自己实现：
>
> ```
> 正常 Topic
>     ↓ 消费失败
> Retry Topic（重试队列）
>     ↓ 重试多次仍失败
> Dead Letter Queue（死信队列）← 存放待人工排查的消息
> ```
>
> > ⚠️ 面试提示：如果需要内置重试功能，可以选 AWS SQS（自带 DLQ）
>
> ------
>
> ## 九、性能优化
>
> ### 三板斧
>
> ```
> 1. 批量发送（Batch）
>    一次 send() 发多条消息 → 减少网络往返
> 
> 2. 消息压缩（Compression）
>    支持 GZIP、Snappy、LZ4 → 消息更小传得更快
> 
> 3. 优化 Partition Key（最重要！）
>    均匀分布 → 最大化并行度
> ```
>
> ------
>
> ## 十、面试答题模板 🎯
>
> ### 引入 Kafka 时的话术
>
> > **"我会引入 Kafka 作为消息队列/事件流，用于[具体原因]。我会用[X]作为 Partition Key 来保证[相关事件的顺序性/均匀分布]。Replication Factor 设为3来保证高可用。"**
>
> ### 被问到扩展性时
>
> > **"Kafka 通过 Partition 实现水平扩展。最关键的决策是 Partition Key 的选择。如果出现热分区，我会考虑[随机加盐/复合Key/取消Key]来解决。"**
>
> ### 被问到消息丢失时
>
> > **"设置 acks=all 确保消息写入所有同步副本后才确认。消费者通过 Offset 机制在重启后恢复，默认是 At-Least-Once 语义。"**
>
> ------
>
> ## 十一、一张图总结
>
> ```
> ┌─────────────────────────────────────────────────────┐
> │                   Kafka 核心架构                      │
> │                                                     │
> │  Producer                                           │
> │  (生产者)  →  Topic（逻辑分类）                       │
> │                    │                                │
> │           ┌────────┴────────┐                       │
> │        Partition 0       Partition 1                │
> │      [msg0,msg1,msg2]  [msg0,msg1]                  │
> │        Broker 1           Broker 2                  │
> │        (Leader)           (Leader)                  │
> │           │                  │                      │
> │      复制到Follower      复制到Follower               │
> │                                                     │
> │  Consumer Group                                     │
> │  Consumer A → Partition 0  (记录Offset)             │
> │  Consumer B → Partition 1  (记录Offset)             │
> └─────────────────────────────────────────────────────┘
> 
> 关键特性：
> ✅ 高吞吐（~100万消息/秒/Broker）
> ✅ 持久性（多副本复制）
> ✅ 有序性（Partition内有序）
> ✅ 扩展性（加Broker+加Partition）
> ⚠️ 大文件 → 存S3，Kafka只存指针
> ⚠️ 消费者重试 → 自己实现DLQ
> ```
>
> ------
>
> ## 快速记忆口诀
>
> - **Topic** = 频道（足球/篮球）
> - **Partition** = 频道内的并行通道，key决定进哪条
> - **Offset** = 书签，消费者的进度记录
> - **Consumer Group** = 团队合作，每个Partition只给一人
> - **Replication** = 备份，Leader挂了Follower顶上
> - **热分区** = 加盐/复合Key/不用Key 来解决
> - **大文件** = 存S3，Kafka存链接

## API Gateway

> ## 一、核心比喻：豪华酒店前台
>
> 想象你住进一家五星级酒店：
>
> ```
> 你（客人）→ 前台（API Gateway）→ 各部门服务
> 
> 前台职责：
> - 验证你的身份（认证）
> - 分配你去哪个房间/部门（路由）
> - 控制访问频率（限流）
> - 统一对外窗口，隐藏内部结构
> ```
>
> **关键洞察**：客人不需要知道"客房部在3楼、餐饮部在1楼、维修部在地下室"，只需要跟前台说就行。同理，客户端不需要知道后端有多少个微服务，只需要跟 API Gateway 说。
>
> ------
>
> ## 二、API Gateway 是什么？
>
> > **一句话定义**：API Gateway 是所有客户端请求的**唯一入口**，负责将请求路由到正确的后端服务，并处理各种通用中间件逻辑。
>
> ```
> 客户端
>    ↓
> [API Gateway] ← 唯一入口
>    ↓        ↓        ↓
> 用户服务  订单服务  支付服务
> ```
>
> **为什么需要它？**
>
> - 没有 API Gateway：客户端需要直接找每个微服务 → 客户端代码复杂、耦合度高
> - 有了 API Gateway：客户端只跟一个地方通信 → 简洁、解耦
>
> ------
>
> ## 三、请求的完整生命周期（最重要！）
>
> ```
> ① 请求进入
>       ↓
> ② 请求验证（格式是否正确？必填字段有没有？）
>       ↓
> ③ 中间件处理（认证？限流？IP黑名单？）
>       ↓
> ④ 路由（发到哪个后端服务？）
>       ↓
> ⑤ 后端处理并返回响应
>       ↓
> ⑥ 响应转换（转成客户端需要的格式）
>       ↓
> ⑦（可选）缓存响应
>       ↓
> ⑧ 返回给客户端
> ```
>
> ### ① 请求验证
>
> - URL 格式是否合法
> - 必须的 Header 是否存在
> - 请求体格式是否正确（如 JSON 格式）
>
> **目的**：在最早期拦截明显错误的请求，不让垃圾请求浪费后端资源。
>
> ------
>
> ### ② 中间件（面试常考三个）
>
> | 中间件                  | 作用         | 举例                      |
> | ----------------------- | ------------ | ------------------------- |
> | **认证 Authentication** | 验证"你是谁" | 检查 JWT Token 是否有效   |
> | **限流 Rate Limiting**  | 防止滥用     | 每用户每分钟最多100次请求 |
> | **IP 白/黑名单**        | 控制访问来源 | 禁止某些恶意IP            |
>
> > ⚠️ **面试技巧**：提到 API Gateway 时，只需说"处理路由和基本中间件"，不要深陷细节。
>
> ------
>
> ### ③ 路由（最核心功能！）
>
> Gateway 维护一张**路由表**：
>
> ```yaml
> routes:
>   - path: /users/*      → 用户服务 (port 8080)
>   - path: /orders/*     → 订单服务 (port 8081)
>   - path: /payments/*   → 支付服务 (port 8082)
> ```
>
> 路由依据可以是：
>
> - URL 路径（最常见）
> - HTTP 方法（GET/POST/DELETE...）
> - Query 参数
> - 请求 Header
>
> **类比**：就像酒店前台看你的需求，说"客房问题去3楼，餐饮问题去1楼"。
>
> ------
>
> ### ④ 响应转换
>
> Gateway 可以做**协议转换**：
>
> ```
> 客户端发送：HTTP GET /users/123/profile
>      ↓（Gateway 转换）
> 内部调用：gRPC → userService.getProfile({userId: "123"})
>      ↓（Gateway 再转换回来）
> 客户端收到：HTTP JSON 响应
> ```
>
> **好处**：内部服务用最高效的协议，外部客户端用统一的标准格式，互不干扰。
>
> ------
>
> ### ⑤ 缓存
>
> 适合缓存的场景：**相同输入 → 相同输出** 且 **不是用户专属数据**
>
> ```
> 第一次请求：/products/热销榜 → 查数据库 → 缓存结果
> 第二次请求：/products/热销榜 → 直接返回缓存 ✅（快！）
> 
> 不适合缓存：/users/123/profile（每个用户数据不同）
> ```
>
> ------
>
> ## 四、扩展方式
>
> ### 水平扩展（最常用）
>
> ```
>            负载均衡器
>           /    |    \
>       GW1    GW2    GW3   ← 多个 Gateway 实例
>         \     |     /
>          后端服务群
> ```
>
> API Gateway 通常是**无状态**的，所以非常适合水平扩展。
>
> ### 全球分布
>
> ```
> 美国用户 → 美国 Gateway
> 欧洲用户 → 欧洲 Gateway   （通过 GeoDNS 自动路由到最近的 Gateway）
> 亚洲用户 → 亚洲 Gateway
> ```
>
> ------
>
> ## 五、什么时候用 API Gateway？
>
> | 场景                           | 用不用？           |
> | ------------------------------ | ------------------ |
> | **微服务架构**（多个后端服务） | ✅ **必用**         |
> | 简单单体应用（一个后端）       | ❌ 不需要，过度复杂 |
> | 单一客户端类型的简单系统       | ❌ 不需要           |
>
> **记忆口诀**：**微服务 → 必有 Gateway；单体应用 → 不需要**
>
> ------
>
> ## 六、常见产品
>
> | 类型   | 产品                     | 特点            |
> | ------ | ------------------------ | --------------- |
> | 云托管 | **AWS API Gateway**      | 与 AWS 深度集成 |
> | 云托管 | **Azure API Management** | 强 OAuth 支持   |
> | 开源   | **Kong**（基于 NGINX）   | 插件生态丰富    |
> | 开源   | **Tyk**                  | 支持 GraphQL    |
>
> ------
>
> ## 七、面试万能话术 🎯
>
> > 当你在设计中引入 API Gateway 时，只需说： **"我会加一个 API Gateway 来处理请求路由和基本的中间件，比如认证和限流。"** 然后继续讲系统的核心部分。
>
> **⚠️ 面试最大误区**：花太多时间讲 API Gateway 的细节 → API Gateway 不是面试的重点，它是"画出来就好"的基础组件。
>
> ------
>
> ## 八、一张图总结
>
> ```
> ┌─────────────────────────────────────────────┐
> │              API Gateway                    │
> │                                             │
> │  ① 验证请求格式                              │
> │  ② 中间件（认证 / 限流 / IP过滤）             │
> │  ③ 路由表 → 找到对应服务                     │
> │  ④ 协议转换（HTTP ↔ gRPC）                  │
> │  ⑤ 响应转换（格式统一）                       │
> │  ⑥ 缓存（可选）                              │
> └─────────────────────────────────────────────┘
>       ↑唯一入口           ↓分发到各微服务
>    所有客户端      用户服务 / 订单服务 / 支付服务
> ```

## Cassandra

## DynamoDB

> 

## PostgresQL

> ## 一、核心比喻：图书馆
>
> 在学任何细节之前，先建立直觉：
>
> ```
> PostgreSQL = 一座超级图书馆
> 
> 书架（Table）      → 存储数据
> 书的目录（Index）  → 快速找到数据
> 图书管理员（Query Planner） → 决定用什么方式找数据
> 借阅记录（Transaction）     → 保证操作的完整性
> 分馆（Replica）    → 分担借阅压力 / 备份
> ```
>
> ------
>
> ## 二、激励性例子：从零到瓶颈
>
> 假设你在设计 **Twitter（X）**，用 PostgreSQL 存储推文：
>
> ```sql
> -- 最简单的设计
> CREATE TABLE tweets (
>   id         BIGSERIAL PRIMARY KEY,
>   user_id    BIGINT,
>   content    TEXT,
>   created_at TIMESTAMP
> );
> ```
>
> **第1天**：100个用户，查询飞快 ✅
>  **第1年**：1亿条推文，`SELECT * WHERE user_id = 123` → 扫描1亿行 💀
>
> **这就是为什么我们需要深入理解 PostgreSQL 的每一层能力。**
>
> ------
>
> ## 三、读性能（Read Performance）
>
> ### 3.1 基础索引（Basic Indexing）
>
> **问题**：没有索引 = 全表扫描（Sequential Scan）
>
> ```
> 没有索引时查 user_id = 123：
> [扫描第1行] → [扫描第2行] → ... → [扫描第1亿行]
> 时间复杂度：O(n) 💀
> ```
>
> **索引本质**：额外维护一个排好序的数据结构，用空间换时间
>
> ```sql
> CREATE INDEX idx_tweets_user_id ON tweets(user_id);
> ```
>
> **默认索引类型：B-Tree**
>
> ```
> B-Tree 索引结构（像字典的目录）：
> 
>          [50]
>         /    \
>     [25]      [75]
>    /    \    /    \
>  [10] [30] [60] [90]
> 
> 查找 user_id=123：
> 从根节点开始，每次排除一半 → O(log n) ✅
> ```
>
> **B-Tree 适合什么查询？**
>
> | 操作     | 例子                        | B-Tree？       |
> | -------- | --------------------------- | -------------- |
> | 等值查询 | `user_id = 123`             | ✅              |
> | 范围查询 | `created_at > '2024-01-01'` | ✅              |
> | 排序     | `ORDER BY created_at`       | ✅              |
> | 模糊查询 | `content LIKE '%hello%'`    | ❌ 全表扫       |
> | 全文搜索 | 查推文内容关键词            | ❌ 需要特殊索引 |
>
> ------
>
> ### 3.2 超越基础索引（Beyond Basic Indexes）
>
> 当 B-Tree 不够用时，PostgreSQL 提供了更强大的武器：
>
> #### **① 复合索引（Composite Index）**
>
> ```sql
> -- 场景：经常这样查
> SELECT * FROM tweets WHERE user_id = 123 ORDER BY created_at DESC;
> 
> -- 解决：把两个字段组合成一个索引
> CREATE INDEX idx_user_time ON tweets(user_id, created_at DESC);
> ```
>
> **⚠️ 重要原则：最左前缀原则**
>
> ```
> 索引 (user_id, created_at)
> 
> ✅ WHERE user_id = 123
> ✅ WHERE user_id = 123 AND created_at > '2024'
> ❌ WHERE created_at > '2024'  ← 跳过了第一列，索引失效！
> ```
>
> **类比**：字典按"姓+名"排序，你能快速找"张X"，但不能直接找"X伟"。
>
> ------
>
> #### **② 部分索引（Partial Index）**
>
> ```sql
> -- 场景：只关心未读通知（大多数通知都是已读的）
> CREATE INDEX idx_unread ON notifications(user_id)
> WHERE is_read = false;
> ```
>
> **好处**：索引只包含一部分行 → 索引更小 → 查询更快 → 占用更少内存
>
> **类比**：图书馆只给"最近一个月的新书"建目录，不给所有书建目录。
>
> ------
>
> #### **③ 全文搜索索引（GIN Index）**
>
> ```sql
> -- 场景：搜索推文内容
> ALTER TABLE tweets ADD COLUMN content_tsv TSVECTOR;
> CREATE INDEX idx_fulltext ON tweets USING GIN(content_tsv);
> 
> -- 查询
> SELECT * FROM tweets WHERE content_tsv @@ to_tsquery('hello & world');
> ```
>
> **GIN vs B-Tree**：
>
> - B-Tree：适合单值查找（数字、日期、字符串前缀）
> - GIN：适合复合值查找（数组、JSON、全文搜索）
>
> ------
>
> #### **④ 覆盖索引（Covering Index）**
>
> ```sql
> -- 查询只需要 user_id 和 created_at，不需要其他字段
> SELECT user_id, created_at FROM tweets WHERE user_id = 123;
> 
> -- 把需要的字段都包进索引（INCLUDE）
> CREATE INDEX idx_covering ON tweets(user_id) INCLUDE (created_at);
> ```
>
> **效果**：查询直接从索引返回结果，**完全不碰原始表** → 极快
>
> ```
> 没有覆盖索引：
> 索引 → 找到行的位置 → 去原始表读数据（额外I/O）
> 
> 有覆盖索引：
> 索引 → 直接返回数据 ✅（省去了去表里读的步骤）
> ```
>
> ------
>
> ### 3.3 查询优化基础（Query Optimization Essentials）
>
> #### Query Planner：图书管理员的决策
>
> PostgreSQL 有个"查询计划器"，它决定**怎么执行你的 SQL**。用 `EXPLAIN ANALYZE` 查看：
>
> ```sql
> EXPLAIN ANALYZE SELECT * FROM tweets WHERE user_id = 123;
> 
> -- 输出示例
> Bitmap Heap Scan on tweets  (cost=4.29..8.30 rows=1)
>   ->  Bitmap Index Scan on idx_tweets_user_id
>         Index Cond: (user_id = 123)
> Execution Time: 0.5ms
> ```
>
> #### 常见扫描类型（从快到慢）
>
> | 扫描类型              | 含义                   | 速度   |
> | --------------------- | ---------------------- | ------ |
> | **Index Only Scan**   | 只读索引，不读表       | 🚀 最快 |
> | **Index Scan**        | 读索引 + 读表对应行    | ✅ 快   |
> | **Bitmap Index Scan** | 批量读取，适合返回多行 | ✅ 中等 |
> | **Sequential Scan**   | 全表扫描               | 💀 慢   |
>
> #### 最常见的查询陷阱
>
> ```sql
> -- ❌ 陷阱1：在索引列上使用函数 → 索引失效
> WHERE LOWER(email) = 'alice@example.com'
> 
> -- ✅ 解决：建函数索引
> CREATE INDEX ON users(LOWER(email));
> 
> -- ❌ 陷阱2：隐式类型转换 → 索引失效
> WHERE user_id = '123'  -- user_id 是 int，'123' 是字符串
> 
> -- ❌ 陷阱3：NOT IN / != → 难以用索引
> -- ✅ 改写成 EXISTS 或 LEFT JOIN
> ```
>
> #### N+1 查询问题（面试高频！）
>
> ```sql
> -- ❌ N+1：先查N个用户，再查每个用户的推文
> SELECT * FROM users;                           -- 1次查询
> SELECT * FROM tweets WHERE user_id = 1;       -- N次查询
> SELECT * FROM tweets WHERE user_id = 2;
> ...
> 
> -- ✅ 用 JOIN 一次解决
> SELECT u.*, t.*
> FROM users u
> JOIN tweets t ON t.user_id = u.id;
> ```
>
> ------
>
> ## 四、写性能（Write Performance）
>
> ### 4.1 写入瓶颈（Throughput Limitations）
>
> PostgreSQL 的写入流程：
>
> ```
> 写入请求
>     ↓
> ① WAL（预写日志）先写到磁盘  ← 保证持久性，但慢！
>     ↓
> ② 写入内存缓冲区（shared_buffers）
>     ↓
> ③ 后台进程异步刷盘
> ```
>
> **WAL（Write-Ahead Log）= 流水账**
>
> > 类比：餐厅先在小票上记录点单，再去做菜。如果做到一半停电了，看小票就能恢复。
>
> **写入性能瓶颈来源：**
>
> | 瓶颈             | 原因                      | 数量级          |
> | ---------------- | ------------------------- | --------------- |
> | **单机写入上限** | 磁盘I/O + WAL同步         | ~几千到几万 TPS |
> | **索引维护开销** | 每次写入都要更新所有索引  | 索引越多越慢    |
> | **锁竞争**       | 并发写同一行时需要加锁    | 高并发下明显    |
> | **MVCC 膨胀**    | 旧版本数据堆积需要 VACUUM | 需要定期清理    |
>
> ------
>
> ### 4.2 写入优化（Write Performance Optimizations）
>
> #### **① 批量写入（Batch Inserts）**
>
> ```sql
> -- ❌ 一条一条插入：每次都触发WAL + 事务开销
> INSERT INTO tweets VALUES (1, 'hello');
> INSERT INTO tweets VALUES (2, 'world');
> 
> -- ✅ 批量插入：一次WAL，一次事务
> INSERT INTO tweets VALUES (1, 'hello'), (2, 'world'), (3, '...');
> 
> -- ✅ 更快：COPY命令（适合大批量导入）
> COPY tweets FROM '/data/tweets.csv' CSV;
> ```
>
> **速度对比**：批量插入比逐行插入快 **10-100倍**
>
> ------
>
> #### **② 索引策略（减少索引数量）**
>
> ```
> 写入时的代价：
> 每条INSERT → 更新表 + 更新所有索引
> 
> 有3个索引 → 每次写入 = 1次表写 + 3次索引写
> 
> 原则：索引是"以写换读"，只建必要的索引！
> ```
>
> ------
>
> #### **③ 分区表（Table Partitioning）**
>
> ```sql
> -- 把大表按时间分成小表
> CREATE TABLE tweets (
>   id         BIGINT,
>   created_at TIMESTAMP
> ) PARTITION BY RANGE (created_at);
> 
> CREATE TABLE tweets_2024 PARTITION OF tweets
>   FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');
> 
> CREATE TABLE tweets_2025 PARTITION OF tweets
>   FOR VALUES FROM ('2025-01-01') TO ('2026-01-01');
> ```
>
> **好处**：
>
> - 查询只扫描相关分区（Partition Pruning）
> - 删除旧数据极快（直接 DROP 分区，不用逐行 DELETE）
> - 每个分区的索引更小，维护更快
>
> ------
>
> #### **④ 连接池（Connection Pooling）**
>
> ```
> 问题：每个数据库连接都很"重"（消耗内存、CPU）
>      1000个并发请求 = 1000个连接 → PostgreSQL 崩溃
> 
> 解决：PgBouncer（连接池中间件）
>      1000个应用连接 → PgBouncer → 20个数据库连接
> ```
>
> > 类比：不是每个客人直接找厨师，而是通过服务员转达，厨师只需要面对少数服务员。
>
> ------
>
> ## 五、复制（Replication）
>
> ### 5.1 架构概览
>
> ```
>                     ┌─────────────┐
> 写入请求 ──────────→ │   Primary   │ (主库，唯一写入点)
>                     └──────┬──────┘
>                            │ WAL 日志流
>               ┌────────────┼────────────┐
>               ↓            ↓            ↓
>          Replica 1    Replica 2    Replica 3
>          (只读)        (只读)        (只读)
> ```
>
> ### 5.2 扩展读性能（Scaling Reads）
>
> **读写分离**：
>
> ```
> 写操作 → Primary
> 读操作 → Replica（任意一个）
> 
> 实际应用：
> - 用户发推文 → Primary
> - 用户刷信息流 → Replica（读多写少，完美！）
> ```
>
> **同步 vs 异步复制：**
>
> | 模式                 | 特点                                 | 代价                               |
> | -------------------- | ------------------------------------ | ---------------------------------- |
> | **异步复制**（默认） | Primary 写完就返回，Replica 稍后同步 | 可能读到旧数据（复制延迟）         |
> | **同步复制**         | Primary 等 Replica 确认后才返回      | 写入延迟增加，但读到的数据是最新的 |
>
> > **面试关键词**：Replica Lag（复制延迟）是异步复制最大的问题，用户刚发的推文可能在 Replica 上查不到。
>
> ------
>
> ### 5.3 高可用（High Availability）
>
> **故障切换（Failover）流程：**
>
> ```
> 正常状态：
> Primary ──写──→ Replica 1（同步）
>          ──写──→ Replica 2（异步）
> 
> Primary 挂掉后（自动切换，需要工具如 Patroni）：
>          Replica 1 → 升级为新 Primary
>          应用更新连接地址
>          原 Primary 恢复后 → 降级为 Replica
> ```
>
> **RPO vs RTO（面试必知）：**
>
> | 指标                                | 含义             | 异步复制的风险           |
> | ----------------------------------- | ---------------- | ------------------------ |
> | **RPO**（Recovery Point Objective） | 最多丢失多少数据 | 异步时可能丢失几秒数据   |
> | **RTO**（Recovery Time Objective）  | 多快能恢复服务   | 自动切换通常几秒到几分钟 |
>
> ------
>
> ## 六、数据一致性（Data Consistency）
>
> ### 6.1 事务（Transactions）
>
> **ACID 特性：**
>
> | 字母  | 英文                  | 含义               | 例子                                  |
> | ----- | --------------------- | ------------------ | ------------------------------------- |
> | **A** | Atomicity（原子性）   | 全成功或全失败     | 转账：扣钱+加钱，要么都成功要么都不做 |
> | **C** | Consistency（一致性） | 事务前后数据合法   | 余额不能为负数                        |
> | **I** | Isolation（隔离性）   | 并发事务互不干扰   | 两人同时转账，不会乱                  |
> | **D** | Durability（持久性）  | 提交后数据永久保存 | 断电也不丢数据（WAL保证）             |
>
> ```sql
> BEGIN;
>   UPDATE accounts SET balance = balance - 100 WHERE id = 1;
>   UPDATE accounts SET balance = balance + 100 WHERE id = 2;
> COMMIT;  -- 两个操作要么都成功，要么都失败
> ```
>
> ------
>
> ### 6.2 隔离级别（Isolation Levels）
>
> **并发问题：**
>
> ```
> 脏读（Dirty Read）：读到另一个事务未提交的数据
> 不可重复读（Non-Repeatable Read）：同一事务两次读结果不同
> 幻读（Phantom Read）：同一查询两次返回的行数不同
> ```
>
> | 隔离级别                   | 脏读  | 不可重复读 | 幻读   | 性能 |
> | -------------------------- | ----- | ---------- | ------ | ---- |
> | Read Uncommitted           | ❌可能 | ❌可能      | ❌可能  | 最高 |
> | **Read Committed（默认）** | ✅防止 | ❌可能      | ❌可能  | 高   |
> | Repeatable Read            | ✅防止 | ✅防止      | ✅防止* | 中   |
> | Serializable               | ✅防止 | ✅防止      | ✅防止  | 最低 |
>
> > PostgreSQL 默认：**Read Committed**，通常够用
>
> ------
>
> ### 6.3 MVCC（多版本并发控制）
>
> PostgreSQL 不用"写锁"来隔离事务，而是用 **MVCC**：
>
> ```
> 写操作时：不修改原数据，而是创建新版本
> 读操作时：读取对应时间点的"快照"
> 
> 用户A 事务中：
>   → 看到的是事务开始时的数据快照
>   → 不管其他事务怎么改，A看到的数据不变
> 
> 效果：读写互不阻塞！
> ```
>
> **代价**：旧版本数据堆积 → 需要 `VACUUM` 定期清理死数据
>
> ------
>
> ## 七、何时用 PostgreSQL？（面试核心！）
>
> ### ✅ PostgreSQL 的甜蜜区间
>
> | 场景                       | 原因                    |
> | -------------------------- | ----------------------- |
> | 需要复杂查询（JOIN、聚合） | 关系型数据库的强项      |
> | 需要 ACID 事务             | 金融、电商、订单系统    |
> | 数据结构清晰有关联         | 用户、订单、商品的关系  |
> | 读多写少 + 读扩展性强      | 配合 Replica 轻松扩展读 |
> | 数据量 < 几TB              | 单机或主从架构足够      |
>
> **典型面试场景**：
>
> - 设计 Twitter（用户、推文、关注关系）
> - 设计电商系统（用户、订单、商品）
> - 设计 Uber（用户、司机、行程记录）
>
> ------
>
> ### ❌ 什么时候考虑替代方案
>
> | 问题                      | 替代方案               | 原因                     |
> | ------------------------- | ---------------------- | ------------------------ |
> | **写入量极大**（百万TPS） | Cassandra / DynamoDB   | 专为写优化，牺牲强一致性 |
> | **需要水平分片写入**      | Cassandra / MongoDB    | PostgreSQL 分片复杂且贵  |
> | **全文搜索**              | Elasticsearch          | 专为搜索优化             |
> | **时序数据**（监控指标）  | TimescaleDB / InfluxDB | 专为时序优化             |
> | **图关系查询**            | Neo4j                  | 复杂关系遍历更高效       |
> | **缓存/会话**             | Redis                  | 内存存储，极快           |
> | **非结构化大数据**        | DynamoDB / MongoDB     | Schema-less 更灵活       |
>
> ### 关键判断框架
>
> ```
> 数据有明确关系？需要JOIN？需要事务？
>     ├── 是 → PostgreSQL ✅
>     └── 否
>          ├── 写入量超大（>万TPS）？→ NoSQL
>          ├── 需要全文搜索？→ Elasticsearch
>          ├── 时序数据？→ TimescaleDB
>          └── 超简单的K-V查询？→ Redis/DynamoDB
> ```
>
> ------
>
> ## 八、容量估算（面试实用）
>
> ```
> 单台 PostgreSQL 的经验数字：
> 
> 存储：几TB（磁盘上限）
> 读QPS：~10,000 - 100,000（加索引、连接池）
> 写QPS：~1,000 - 10,000
> 连接数：~几百个（需要 PgBouncer）
> 复制延迟：通常 < 100ms（异步）
> 
> 超过这些 → 考虑分片（Sharding）或换方案
> ```
>
> ------
>
> ## 九、完整知识地图
>
> ```
> ┌──────────────────────────────────────────────────────┐
> │                   PostgreSQL                          │
> │                                                      │
> │  读性能                        写性能                 │
> │  ├── B-Tree索引（默认）         ├── WAL 保证持久性      │
> │  ├── 复合索引（多字段）          ├── 批量写入（10-100x）│
> │  ├── 部分索引（过滤条件）        ├── 减少索引数量        │
> │  ├── 覆盖索引（不碰原表）        ├── 分区表（大表拆小）  │
> │  ├── GIN（全文/数组/JSON）       └── 连接池（PgBouncer）│
> │  └── EXPLAIN ANALYZE（调试）                          │
> │                                                      │
> │  复制                          一致性                 │
> │  ├── Primary-Replica 架构      ├── ACID 事务           │
> │  ├── 读写分离                   ├── 隔离级别            │
> │  ├── 异步（快）vs 同步（准）    └── MVCC（读写不阻塞）  │
> │  └── 自动故障切换（Patroni）                          │
> │                                                      │
> │  用PostgreSQL：关系型数据、事务、复杂查询              │
> │  不用PostgreSQL：超高写入、全文搜索、时序、图数据       │
> └──────────────────────────────────────────────────────┘
> ```
>
> ------
>
> ## 十、面试万能话术 🎯
>
> **引入 PostgreSQL 时：**
>
> > "我会用 PostgreSQL 作为主数据库，它能满足我们对事务和复杂查询的需求。读压力大时，我会添加只读 Replica 做读写分离。"
>
> **被问到性能时：**
>
> > "首先确保热查询字段有合适的索引，尤其是复合索引要注意最左前缀原则。写入瓶颈时考虑批量写入、减少索引数量、或者引入连接池 PgBouncer。"
>
> **被问到扩展性时：**
>
> > "PostgreSQL 的读扩展靠 Replica，写扩展比较难。如果写入量超过单机上限，需要考虑 Sharding，或者重新评估是否该换 Cassandra/DynamoDB。"

## Flink

## Zookeeper

# Core Concepts

## Networking Essentials

### Networking 101

网络的本质是**让设备能互相通信**。为了简化复杂性，网络被分成多个层，每层只负责一件事，上层不需要关心下层的实现细节。

> 类比：你用冰箱存食物，不需要知道压缩机原理；你用HTTP发请求，不需要知道电信号怎么传输。

------

**三个关键层（面试最常考）**

| 层级    | 名称   | 代表协议                   | 职责一句话                                       |
| ------- | ------ | -------------------------- | ------------------------------------------------ |
| Layer 3 | 网络层 | **IP**                     | 给数据打上地址，负责在全球路由（找到目标服务器） |
| Layer 4 | 传输层 | **TCP / UDP / QUIC**       | 保证数据可靠、有序地到达                         |
| Layer 7 | 应用层 | **HTTP / DNS / WebSocket** | 定义数据的具体格式和用途                         |

> 类比：IP = 快递地址，TCP = 保证签收回执，HTTP = 信封里信的格式。

------

**一次完整Web请求的流程**

输入 `hellointerview.com` 回车后：

```
1. DNS解析        → 把域名转成IP（如 32.42.52.62）
2. TCP三次握手    → 建立可靠连接
   client → SYN
   server → SYN-ACK
   client → ACK
3. HTTP请求       → 发送 GET 请求
4. 服务器处理     → 这是你作为开发者唯一能控制的延迟！
5. HTTP响应       → 返回数据
6. TCP四次挥手    → 关闭连接
   client → FIN
   server → ACK
   server → FIN
   client → ACK
```

------

**面试必记的三个核心观点**

**① 开发者可以忽略底层细节** TCP帮你保证数据可靠传输，IP帮你找到目标服务器。你只需要关心应用层逻辑。

**② 一个"请求"背后有很多次数据包交换** 层级越高 → 延迟越大、处理越多。这在设计负载均衡器时很关键。

**③ TCP连接是有状态的，维护成本不低** 每次请求都要握手+挥手，代价大。所以引入了：

- **HTTP keep-alive**：复用连接
- **HTTP/2 多路复用**：一个连接并发多个请求

这对设计**实时系统**（如聊天、直播）非常重要。

### Network Layer Protocols

IP 协议的核心任务就两个字：**寻址**。

- **面试掌握程度**：在系统设计中，你只需要知道 IP 负责“把数据包路由到正确的机器上”就足够了，不用去死磕底层的路由算法。

### Transport Layer Protocols

传输层的作用：把IP层"尽力而为"的数据包，变成应用能用的可靠通信。

------

**UDP — 快但不可靠**

> 类比：机关枪扫射，不管有没有打中。

- **无连接**：不握手，直接发
- **不保证送达**：包可能丢
- **不保证顺序**：包可能乱序
- **优点**：极低延迟，开销小

**适用场景**：实时性 > 可靠性

| 场景          | 原因                     |
| ------------- | ------------------------ |
| 直播/视频通话 | 丢一帧没关系，延迟不能高 |
| 在线游戏      | 实时操作，容忍偶尔丢包   |
| DNS查询       | 小包，快速，丢了重发就行 |
| VoIP语音      | 偶尔卡一下比全程延迟好   |

⚠️ **面试注意**：浏览器不原生支持UDP（除了WebRTC）。如果设计中用了UDP，要说明浏览器用户的备选方案（如降级为HTTP批量推送）。

------

**TCP — 可靠但有开销**

> 类比：挂号信，必须签收确认，丢了重寄。

- **面向连接**：三次握手建立连接
- **保证送达且有序**：丢包自动重传
- **流量控制**：防止接收方被淹没
- **拥塞控制**：网络拥堵时自动降速

**适用场景**：几乎所有需要数据完整性的场景（HTTP、数据库、文件传输等）

------

**TCP vs UDP 对比表**

| 特性       | UDP              | TCP          |
| ---------- | ---------------- | ------------ |
| 连接方式   | 无连接           | 需要握手     |
| 可靠性     | 尽力而为         | 保证送达     |
| 顺序       | 不保证           | 保证         |
| 速度       | 快               | 慢（有开销） |
| Header大小 | 8字节            | 20-60字节    |
| 典型场景   | 直播、游戏、VoIP | 其他所有     |

------

**面试策略**

**默认用TCP，无需解释。**

想加分？主动提出UDP更合适的场景，并说清楚：

1. 为什么这里低延迟比可靠性重要
2. 如何处理丢包问题
3. 浏览器用户怎么办

> 实际案例：视频会议 → TCP做登录/信令 + UDP/WebRTC做音视频流

------

**QUIC 加分项（了解即可）**

QUIC = TCP的现代化替代品，更快（减少握手次数），基于UDP实现，HTTP/3用的就是它。提到它会让面试官印象深刻，但不必深入展开。

### Application Layer Protocols

应用层 = 开发者日常打交道的层。运行在"用户空间"（可灵活修改），底层跑在OS内核（稳定高效）。

**HTTP = 无状态的请求-响应协议**

> 无状态 = 服务器不记得你上一次请求是什么。这是好事，减少了系统复杂度。

常用请求方法

| 方法   | 用途     | 备注         |
| ------ | -------- | ------------ |
| GET    | 获取数据 | 无body，幂等 |
| POST   | 创建资源 | 有body       |
| PUT    | 全量更新 | 幂等         |
| PATCH  | 局部更新 |              |
| DELETE | 删除资源 | 幂等         |

常用状态码

| 代码    | 含义                |
| ------- | ------------------- |
| 200     | 成功                |
| 201     | 创建成功            |
| 301/302 | 重定向（永久/临时） |
| 401     | 未认证              |
| 403     | 无权限              |
| 404     | 资源不存在          |
| 429     | 请求过多（限流）    |
| 500     | 服务器内部错误      |
| 502     | 网关错误            |

**HTTPS** = HTTP + TLS加密，传输内容安全，但⚠️不代表请求body可信，服务端必须验证参数（如user_id）！

------

**三大API范式**

REST — 默认选择

**核心思想**：操作"资源"，而非"动作"

```
GET    /users/{id}         → 获取用户
POST   /users              → 创建用户
PUT    /users/{id}         → 更新用户
DELETE /users/{id}         → 删除用户
GET    /users/{id}/posts   → 获取用户的帖子
```

❌ 别这样想：`updateUser()`, `startGame()`（这是方法，不是资源） ✅ 这样想：`PUT /users/{id}`, `PATCH /games/{id}` with `{"status": "started"}`

**优点**：简单、通用、易理解
 **缺点**：高吞吐时性能不是最优，JSON序列化有开销
 **面试建议**：**默认用REST**，除非有特殊需求再换。

------

**GraphQL — 灵活查询**

**解决什么问题**：REST容易出现两个痛点：

- **Under-fetching**：一个页面要发10个请求才能拼齐数据（多次往返，高延迟）
- **Over-fetching**：API返回一堆用不到的字段（浪费带宽）

**GraphQL解法**：客户端自己指定要哪些字段，服务端只返回这些。

```json
query {
  user(id: "123") {
    username
    profile { avatar }
    groups { name }
  }
}
```

**适用场景**：前端需要灵活迭代、多团队共用同一后端、移动端需要省流量
**面试建议**：只在需求"灵活多变"时提出，大多数面试用REST更好。

------

**gRPC — 高性能内部通信**

**核心优势**：用 Protocol Buffers（二进制）替代JSON，体积更小、解析更快（可达10倍吞吐量提升）

```protobuf
// JSON: 40字节
{"id": "123", "name": "John Doe"}

// Protobuf: 15字节（二进制）
0A 03 31 32 33 12 08 6A 6F 68 6E 20 64 6F 65
```

**适用场景**：微服务内部通信、性能敏感、数据量大
 **不适合**：公开API（浏览器不支持，工具链不成熟）
 **面试建议**：外部API用REST，内部服务间通信可以提gRPC加分。

------

**实时推送：SSE vs WebSocket vs WebRTC**

**SSE — 服务端单向推送**

**原理**：普通HTTP连接，但服务器持续发送"数据块"，客户端逐条处理。

```json
data: {"event": "price_update", "price": 100}
data: {"event": "price_update", "price": 105}
```

**优点**：基于HTTP，简单，自动重连
 **缺点**：单向（只能服务器→客户端），连接会被断开，某些网络会批处理导致失效
 **适用场景**：拍卖实时报价、通知推送、AI流式输出

------

**WebSocket — 双向实时通信**

**原理**：通过HTTP升级协议，建立持久TCP连接，双方随时可发消息。

```
HTTP Upgrade → WebSocket连接建立 → 双向自由发消息
```

**优点**：真正的双向通信，低延迟
 **缺点**：有状态连接，基础设施成本高（防火墙、负载均衡都要支持）
 **适用场景**：在线游戏、聊天应用、协同编辑
 **面试警告**：⚠️ 别乱用！没有充分理由就用WebSocket会被扣分。只在真正需要高频双向通信时使用。

------

**WebRTC — 点对点通信**

**原理**：浏览器之间直接通信，不经过服务器中转数据（但需要信令服务器建立连接）。

连接建立4步：

1. 连接信令服务器，发现对端
2. 通过STUN服务器获取自己的公网IP:Port
3. 通过信令服务器交换连接信息
4. 直接建立P2P连接传输数据

**唯一使用UDP的应用层协议！**
 **适用场景**：视频/音频通话、会议（仅此！）
 **面试建议**：只用于视频会议，其他场景别碰，容易翻车。

------

**选型速查表**

| 场景                             | 推荐协议  |
| -------------------------------- | --------- |
| 普通Web API（默认）              | REST      |
| 前端灵活查询、多团队协作         | GraphQL   |
| 内部微服务高性能通信             | gRPC      |
| 服务端单向推送（通知、直播弹幕） | SSE       |
| 双向实时通信（游戏、聊天）       | WebSocket |
| 视频/音频通话                    | WebRTC    |

### Load Balancing

水平扩展（加更多服务器）之后，客户端怎么知道该找哪台服务器？→ **负载均衡**

------

**两种负载均衡方式**

**客户端负载均衡**

客户端自己决定找哪台服务器。

**流程**：客户端 → 查询服务注册中心 → 直接访问目标服务器

**优点**：少一次网络跳转，速度快
 **缺点**：客户端需要定期同步服务器列表，更新有延迟

**经典例子**：

- **Redis Cluster**：客户端拿到集群节点信息，自己hash决定找哪个节点
- **DNS**：DNS返回IP列表，每次顺序不同，客户端自动打到不同服务器（同时解决负载均衡器单点故障问题：设两个LB，DNS轮询）

**面试适用场景**：内部微服务通信（gRPC内置支持），其他情况用专用负载均衡器。

------

**专用负载均衡器**

独立的一台服务器/设备，客户端只需要知道它的地址，后端有几台服务器对客户端透明。

**代价**：每次请求多一次跳转
 **收益**：服务器列表实时更新，路由策略灵活

------

**L4 vs L7 负载均衡器**

|          | L4（传输层）                | L7（应用层）                    |
| -------- | --------------------------- | ------------------------------- |
| 工作层级 | TCP/UDP                     | HTTP                            |
| 看什么   | IP + 端口                   | 请求内容（URL、Header、Cookie） |
| 速度     | 快（几乎不检查包内容）      | 较慢（需解析应用层）            |
| 连接方式 | 维持客户端-服务器TCP连接    | 自己终止连接，再新建连接到后端  |
| 适用场景 | **WebSocket**、需要持久连接 | **HTTP**、需要按内容路由        |

> 类比：L4 = 看收件地址分拣快递；L7 = 打开包裹看内容再决定送哪

**面试结论**：用了WebSocket → L4；其他HTTP场景 → L7。

------

**健康检查**

负载均衡器会定期探测后端服务器是否存活，挂了的自动摘除流量。

- **TCP健康检查**：能不能建立连接
- **HTTP健康检查**：返回200才算健康，500或超时就摘除

------

**负载均衡算法**

| 算法                         | 说明                      | 适用场景                                |
| ---------------------------- | ------------------------- | --------------------------------------- |
| 轮询 Round Robin             | 依次分配                  | 默认，无状态服务                        |
| 随机 Random                  | 随机选                    | 无状态服务                              |
| 最少连接 Least Connections   | 找当前连接数最少的        | **SSE/WebSocket**（防止连接堆积在单台） |
| 最快响应 Least Response Time | 找响应最快的              | 性能敏感                                |
| IP Hash                      | 同一客户端IP → 同一服务器 | 需要会话粘性                            |

**记住**：SSE/WebSocket这种持久连接 → 用 **Least Connections**，否则随着时间推移一台服务器会积累大量连接。

------

**面试速记**

```
默认：水平扩展 + L7负载均衡器 + 轮询
WebSocket：L4负载均衡器 + Least Connections
内部微服务：客户端负载均衡（gRPC自带）
避免LB单点故障：双LB + DNS轮询
```

### Common Deep Dives and Challenges

三大核心模式

**1. 超时 + 重试 + 退避 (Timeout + Retry + Backoff)**

**超时**：请求超过预期时间就放弃，别无限等待。

**重试**：失败了就再试，适合处理**瞬时故障**（服务器临时慢）。

**前提**：API必须是幂等的（见下一节），否则重试会出问题。

**退避（Backoff）**：不能立刻重试，要等一段时间再试。否则所有客户端同时狂轰，反而把已经虚弱的服务器打死。

**指数退避 + Jitter（随机抖动）**：

```
第1次失败 → 等1s（+随机）再试
第2次失败 → 等2s（+随机）再试
第3次失败 → 等4s（+随机）再试
...
```

加随机抖动是为了防止所有客户端"同步重试"，形成踩踏效应。

> 面试魔法词：**"retry with exponential backoff and jitter"**，说出来必加分。

------

**2. 幂等性 (Idempotency)**

**幂等** = 同一请求执行多次，结果和执行一次完全相同。

**为什么重要**：重试的前提是操作幂等，否则：

- 支付接口重试2次 → 用户被扣了3次钱 💸

**天然幂等**：GET（读数据不改变状态）

**写操作如何幂等 → 幂等键（Idempotency Key）**：

客户端生成唯一ID（如 `user_id + date`），每次请求带上。服务端检查：

- 没见过这个key → 处理，记录key
- 见过了 → 直接返回之前的结果，不重复执行

```
POST /payments
Idempotency-Key: user123-2026-02-20
{ "amount": 10 }
```

------

**3. 熔断器 (Circuit Breaker)**

**解决问题**：一个服务挂了，大量重试反而让它更难恢复（雪崩/Thundering Herd）。

**类比**：家里电路过载时，保险丝自动断开，保护整个电路系统。

**三种状态**：

```
正常            故障超过阈值         等待一段时间
[Closed] ──失败太多──→ [Open] ──超时后──→ [Half-Open]
   ↑                                          │
   └────────── 测试请求成功 ──────────────────┘
               测试失败 → 回到 Open
```

| 状态                  | 行为                                       |
| --------------------- | ------------------------------------------ |
| **Closed（关闭）**    | 正常通过请求                               |
| **Open（断开）**      | 立即拒绝请求，不真正发出调用               |
| **Half-Open（半开）** | 放一个测试请求，成功则恢复，失败则继续断开 |

**优点**：

- 快速失败，不让用户干等
- 保护下游服务，给它恢复时间
- 防止级联故障（一个服务挂掉搞垮整个系统）

**适用场景**：调用第三方API、数据库连接、微服务间通信。

## API Design

### API Types

```
默认 → REST
客户端需要灵活查询/避免over-fetching → GraphQL
内部微服务高性能通信 → gRPC (RPC)
实时推送/双向通信 → SSE / WebSocket（见实时更新章节）
```

------

**REST — 深入掌握**

**资源建模**

REST的核心：**操作"名词"（资源），不是"动词"（动作）**

```
✅ GET  /events/123/tickets     → 获取某活动的票
✅ POST /events/123/bookings    → 创建订单
❌ POST /bookEvent              → 不RESTful
```

资源用**复数名词**：`/events`, `/bookings`, `/users`

**嵌套 vs 查询参数的选择**：

- 关系**必须**存在 → 路径参数：`/events/123/tickets`（必须指定哪个活动）
- 关系**可选** → 查询参数：`/tickets?event_id=123&section=VIP`

------

**HTTP方法与幂等性**

| 方法   | 用途          | 幂等？                   |
| ------ | ------------- | ------------------------ |
| GET    | 读取数据      | ✅                        |
| PUT    | 全量替换/创建 | ✅                        |
| DELETE | 删除          | ✅                        |
| POST   | 创建新资源    | ❌（重复调用 = 重复创建） |
| PATCH  | 局部更新      | 看实现                   |

> 幂等性关键：网络失败重试时，GET/PUT/DELETE安全，POST不安全（会重复创建）。

------

**传参三种方式**

```json
POST /events/123/bookings?notify=true
       ↑路径参数          ↑查询参数
{
  "tickets": [{"section": "VIP", "quantity": 2}]
}   ↑ 请求体
```

| 位置         | 用途                   | 示例               |
| ------------ | ---------------------- | ------------------ |
| **路径参数** | 标识"哪个资源"（必须） | `/events/123`      |
| **查询参数** | 可选过滤/排序/分页     | `?page=2&limit=20` |
| **请求体**   | 创建/更新的复杂数据    | JSON对象           |

------

**状态码（记住分类即可）**

| 分类 | 含义       | 常用                                                |
| ---- | ---------- | --------------------------------------------------- |
| 2xx  | 成功       | 200成功，201已创建                                  |
| 3xx  | 重定向     | 301永久，302临时                                    |
| 4xx  | 客户端错误 | 400参数错，401未登录，403无权限，404不存在，429限流 |
| 5xx  | 服务端错误 | 500内部错误，502网关错误                            |

> 面试重点：能区分4xx（客户端的锅）和5xx（服务端的锅）就够了。

------

**GraphQL — 何时使用**

**触发条件**：面试官说"不同客户端需要不同数据"、"避免over/under-fetching"。

**核心机制**：单一端点，客户端自定义返回字段。

```sql
query {
  event(id: "123") {
    name          # 只要这些字段
    venue { name }
    tickets { price }
  }
}
```

**⚠️ N+1问题**（面试加分项）：查询100个活动各自的场馆 = 101次数据库查询。解决方案：DataLoader批处理。

**适用**：多端（移动端/Web端）数据需求差异大、前端需要快速迭代
 **不适用**：大多数面试场景，REST更简洁

------

**gRPC — 内部服务通信**

**核心优势**：Protocol Buffers（二进制）+ HTTP/2 = 比REST快10倍

```sql
service TicketService {
  rpc CreateBooking(CreateBookingRequest) returns (Booking);
}
```

自动生成多语言客户端代码，编译期类型检查。

**适用**：微服务内部通信、性能敏感、多语言环境
 **不适用**：公开API（浏览器不支持）

**面试策略**：公开API用REST，内部服务间通信可提gRPC。通常面试只需设计面向用户的API，内部API点一句"通过gRPC通信"即可。

------

**一张图总结**

```
用户/浏览器/移动端
       ↓ REST (HTTP/JSON)
   [公开API层]
       ↓ gRPC (Protobuf/HTTP2)
[内部微服务 A] ←→ [内部微服务 B]
```

### Common API Patterns

**分页**

大数据集不能一次返回全部，必须分页。

**Offset分页**（简单，默认用）：

```json
GET /events?offset=20&limit=10   # 取第21-30条
```

缺点：数据实时变化时可能重复或漏掉。

**Cursor分页**（实时数据用）：

```json
# 第一次请求
GET /events?limit=10
→ { "events": [...], "next_cursor": "abc123" }

# 下一页
GET /events?cursor=abc123&limit=10
```

Cursor = 指向上一页最后一条记录的指针，新增数据不影响翻页结果。

> **面试策略**：默认Offset分页。实时数据/高并发场景提Cursor。关键是**记得加分页**，具体方案其次。

### Security Considerations

**认证 vs 授权**

| 概念                    | 问题         | 例子                                               |
| ----------------------- | ------------ | -------------------------------------------------- |
| **认证 Authentication** | 你是谁？     | 验证是 [john@example.com](mailto:john@example.com) |
| **授权 Authorization**  | 你能干什么？ | 只能取消自己的订单                                 |

------

**API Key vs JWT**

**API Key**：随机长字符串，存数据库验证

- ✅ 服务器间通信、开放给第三方开发者
  - Proves the identity of the application or developer making the request, ensuring it's a legitimate user. 
- ❌ 不适合面向用户的产品（用户不该管理密钥）

```
Authorization: Bearer sk_live_abc123...
```

**JWT（JSON Web Token）**：把用户信息编码进token，用密钥签名

- ✅ 无需数据库查询（自包含），天然适合分布式系统
- ✅ 面向用户的Web/移动应用

```json
{ "user_id": "123", "role": "customer", "exp": 1640995200 }
```

> **面试策略**：说"用JWT认证用户身份，内部服务间用API Key"就够了。

---

**RBAC（基于角色的访问控制）**

- customer      → 只能查/取消自己的订单
- venue_manager → 可以创建活动、查看销售报告
- admin         → 全部权限

面试中只需说明"哪些端点需要哪个角色"，不需要深入实现。

---

**限流（Rate Limiting）**

超出请求限制 → 返回 **429 Too Many Requests**

常见策略：按用户、按IP、按端点分别限制。面试中说一句"在API Gateway层加限流防止滥用"即可。

## Data Modeling

> ## 一、什么是 Data Modeling？
>
> 决定你的数据**长什么样、存在哪、互相怎么关联**。
>
> 面试中不需要完美，只需要：清晰、能用、符合需求。
>
> ------
>
> ## 二、选哪种数据库？（默认选 SQL）
>
> | 数据库类型        | 代表       | 什么时候用                     |
> | ----------------- | ---------- | ------------------------------ |
> | **关系型（SQL）** | PostgreSQL | **默认选这个**，99%情况够用    |
> | 文档型            | MongoDB    | schema 经常变化                |
> | 键值型            | Redis      | 缓存、session，配合SQL用       |
> | 宽列型            | Cassandra  | 海量写入、时序数据、日志       |
> | 图数据库          | Neo4j      | 几乎不用，听起来高级但加分不多 |
>
> > 💡 **面试原则**：除非需求明确指向其他类型，否则直接用 PostgreSQL。
>
> ------
>
> ## 三、Schema 设计步骤
>
> **第一步：从需求出发，确认三件事**
>
> 1. **数据量** → 决定是否需要分库分表
> 2. **访问模式（最重要）** → 你的 API 需要什么查询？以此决定索引和结构
> 3. **一致性要求** → 支付等场景要强一致性（用SQL的ACID）；动态Feed允许最终一致性
>
> ------
>
> **第二步：定义实体与关系**
>
> ```
> users:    id(PK), username, email
> posts:    id(PK), user_id(FK→users), content, created_at
> comments: id(PK), post_id(FK→posts), user_id(FK→users), content
> likes:    user_id(FK), post_id(FK)
> ```
>
> - **PK（主键）**：唯一标识一条记录，用系统生成的ID，不用邮箱等业务字段
> - **FK（外键）**：表示关联关系，防止出现"指向不存在用户的帖子"
> - 关系类型：一对多（用户→帖子）、多对多（用户↔点赞）
>
> ------
>
> **第三步：加索引**
>
> 索引 = 书的目录，让数据库不用扫全表就能快速找到数据。
>
> ```
> -- 查某用户的帖子
> INDEX on posts.user_id
> 
> -- 按时间排序
> INDEX on posts.created_at
> 
> -- 查某用户最近的帖子（复合索引）
> INDEX on (user_id, created_at)
> ```
>
> > 💡 面试技巧：每个 API endpoint 需要什么查询，就加对应的索引。
>
> ------
>
> **第四步：规范化 vs 反规范化**
>
> - **规范化（Normalized）**：每条数据只存一份，修改不会出现不一致 → **默认做法**
> - **反规范化（Denormalized）**：为了读取快，把数据冗余复制 → 会带来更新不一致问题
>
> ```
> ❌ 反规范化的坏处：
> 用户改了用户名 → 要更新他所有帖子里冗余的用户名字段 → 漏掉就数据不一致
> ```
>
> > 💡 需要快速读取但又不想破坏规范化？在数据库前加 **Redis 缓存**，源数据保持规范化，缓存里放预计算好的数据。
>
> ------
>
> **第五步：Sharding（数据量大时才需要）**
>
> 数据太多，一台机器放不下 → 按某个 key 拆分到多台机器。
>
> - **选 Shard Key 原则**：按主要查询模式来
>   - 经常查"某用户的帖子" → 按 `user_id` 分片，同一用户的数据在同一台机器
> - **时间分片是坑**：当前时间段的写入都打到同一台机器 → **热点问题**
> - **尽量避免跨片查询**：跨机器查询非常慢且复杂
>
> ------
>
> ## 四、面试回答框架
>
> ```
> 1. 需求阶段：列出核心实体（User / Post / Like...）
> 2. 高层设计阶段：
>    - 选数据库类型（通常是 PostgreSQL）
>    - 列出每个表的关键字段
>    - 标注 PK / FK
>    - 说明需要哪些索引（结合API）
>    - 说明是否需要反规范化（结合一致性需求）
>    - 数据量大时说明 Shard Key 选择
> ```
>
> ------
>
> ## 五、一句话记忆法
>
> > **默认SQL → 定实体/关系 → 按API加索引 → 保持规范化（需要快就加缓存）→ 量大再考虑Sharding**

## Caching

> # 系统设计面试：Caching 核心笔记
>
> ------
>
> ## 一、为什么要用缓存？
>
> 数据库读取 ≈ **50ms**（磁盘） Redis 读取 ≈ **1ms**（内存）
>
> **快50倍**。缓存的本质就是：把常用数据放到内存里，跳过慢的数据库。
>
> ------
>
> ## 二、缓存放在哪？（4个位置）
>
> | 位置                   | 例子       | 什么时候用                                        |
> | ---------------------- | ---------- | ------------------------------------------------- |
> | **外部缓存（最重要）** | Redis      | 读多写少的高流量系统，**面试默认说这个**          |
> | **CDN**                | Cloudflare | 图片/视频等静态媒体，地理分布式用户               |
> | **客户端缓存**         | 浏览器缓存 | 减少重复网络请求                                  |
> | **进程内缓存**         | 应用内存   | 极热key、配置项、feature flag（作为优化层补充提） |
>
> ------
>
> ## 三、4种缓存读写模式
>
> ### ✅ Cache-Aside（最常用，面试默认选这个）
>
> ```
> 读：先查缓存 → 命中就返回 → 未命中就查数据库 → 存入缓存 → 返回
> 写：直接写数据库 → 删除/失效缓存
> ```
>
> 只在需要时才缓存，缓存保持精简。缓存miss时有一次额外延迟。
>
> ------
>
> ### Write-Through
>
> ```
> 写：先写缓存 → 缓存同步写数据库 → 完成
> ```
>
> 保证缓存和数据库一致，但写操作慢（要等两边都写完）。
>
> ------
>
> ### Write-Behind（Write-Back）
>
> ```
> 写：只写缓存 → 缓存异步批量写数据库
> ```
>
> 写入极快，但缓存崩溃可能丢数据。适合允许丢失的场景（日志/分析）。
>
> ------
>
> ### Read-Through
>
> ```
> 读：查缓存 → Miss时缓存自己去查数据库 → 存入 → 返回（应用不直接查数据库）
> ```
>
> CDN就是这个模式。应用层很少用，了解即可。
>
> ------
>
> ## 四、缓存淘汰策略（Eviction Policy）
>
> 缓存满了该删谁？
>
> - **LRU**（最近最少使用）→ **默认选这个**，适合大多数场景
> - **LFU**（访问频率最低）→ 适合有长期热门数据的场景（如热门视频）
> - **TTL**（设置过期时间）→ 必须配合上面一起用，防止数据过时
>
> > 💡 面试标准答案：**LRU + TTL 组合使用**
>
> ------
>
> ## 五、三大常见问题（会被面试官追问）
>
> ### 1. Cache Stampede（缓存雪崩/惊群效应）
>
> **问题**：热门缓存key同时过期 → 大量请求同时打到数据库 → 数据库崩溃
>
> **解决**：
>
> - **Request Coalescing（合并请求）**：只让一个请求去查数据库，其他等结果 ✅ 最有效
> - **Cache Warming**：提前刷新快过期的key
>
> ------
>
> ### 2. Cache Consistency（缓存一致性）
>
> **问题**：数据库更新了，缓存还是旧数据
>
> **解决**：
>
> - **写数据库后立即删除缓存**（下次读时自动重建）✅ 最常用
> - **设短TTL**：允许短暂不一致的场景用这个
> - **接受最终一致性**：Feed流、点赞数允许几秒延迟
>
> ------
>
> ### 3. Hot Keys（热点key）
>
> **问题**：某个key（如Taylor Swift的主页）流量极大，打爆单个Redis节点
>
> **解决**：
>
> - **复制热key到多个节点**，负载均衡读取
> - **加进程内缓存**，极热数据不走Redis
>
> ------
>
> ## 六、面试回答框架（5步走）
>
> ```
> 1. 指出瓶颈
>    "用户profile查询每秒500次，每次30ms，数据库成为瓶颈"
> 
> 2. 决定缓存什么
>    "缓存用户profile（读多改少）和trending feed（计算昂贵）"
>    → 想好 cache key：user:123:profile
> 
> 3. 选择架构
>    "用 Cache-Aside，读时先查Redis，miss再查数据库"
>    → 有静态媒体加CDN，极热key加进程内缓存
> 
> 4. 设置淘汰策略
>    "LRU + TTL 10分钟，用户更新时主动失效对应key"
> 
> 5. 说明缺点和应对
>    选1-2个相关问题说：一致性怎么处理？Redis挂了怎么办？热key怎么处理？
> ```
>
> ------
>
> ## 七、什么时候提出缓存？
>
> 先找到问题，再提出缓存。触发条件：
>
> - 读流量大（DAU × 每人请求数 = 天文数字）
> - 查询昂贵（多表join、复杂聚合）
> - 需要低延迟（要求<10ms，数据库要30ms）
> - 数据库CPU高
>
> > ⚠️ 不要一上来就说"加个Redis"——要先说清楚为什么需要它。
>
> ------
>
> ## 八、一句话记忆法
>
> > **读慢就Cache-Aside + Redis → LRU+TTL管内存 → 写后失效key保持一致 → 注意雪崩/热key两大坑**

## Sharding

**核心问题：单台数据库撑不住了怎么办？**

**垂直扩展**（升级更强的机器）有上限，最终必须：

> **把数据拆分到多台机器上** → 这就是 Sharding

------

**Partitioning vs Sharding 区别**

|            | Partitioning | Sharding        |
| ---------- | ------------ | --------------- |
| 数据在哪里 | 同一台机器内 | 多台独立机器    |
| 解决什么   | 查询效率     | 存储/吞吐量瓶颈 |

------

**三种 Sharding 策略**

**1. Range-Based（按范围）**

```
Shard 1 → user_id 1~100万
Shard 2 → user_id 100万~200万
```

✅ 范围查询快
 ❌ 容易热点（比如按时间分片，新数据全涌向最新 Shard）

------

**2. Hash-Based（按哈希）⭐ 默认首选**

```
shard = hash(user_id) % 4
```

✅ 分布均匀
 ❌ 增减 Shard 时需要迁移大量数据（用 **Consistent Hashing** 解决）

------

**3. Directory-Based（查表）**

```
查询映射表：user_15 → Shard 3
```

✅ 最灵活，可以随时调整
 ❌ 每次请求多一次查表，映射表宕机全系统崩溃
 ⚠️ 面试中几乎不用，除非特殊需求

------

**好的 Shard Key 三个条件**

1. **高基数** — 有足够多不同的值（user_id ✅，布尔值 ❌）
2. **分布均匀** — 不能90%数据在一个 Shard
3. **符合查询模式** — 常见查询只打一个 Shard

------

**三大挑战 & 解决方案**

**❶ 热点问题（Hot Spot）**

> 某个 Shard 流量远超其他（如明星用户）

解决：把热点 key 单独移到专属 Shard（此时用 Directory-Based）

------

**❷ 跨 Shard 查询（Cross-Shard）**

> "全平台最热帖子" → 要查所有 Shard，很慢

解决：

- **缓存**结果（不需要实时精确）
- **反规范化**：把常用数据冗余存在同一个 Shard
- 低频查询直接接受慢

------

**❸ 跨 Shard 事务（Consistency）**

> 用户A在Shard1，用户B在Shard2，转账怎么保证原子性？

解决：

- **设计上避免**：把一个用户所有数据放同一 Shard
- **Saga 模式**：拆成多步，每步有补偿操作（失败就回滚）
- **接受最终一致性**：短暂不一致可接受就别用分布式事务

------

**面试答题模板**

```
1. 先证明需要 Sharding（算容量/QPS）
2. 提出 Shard Key："我按 user_id 分片，因为大多数查询都是用户维度的"
3. 选策略："用 Hash-Based + Consistent Hashing，分布均匀且扩容方便"
4. 说出 Trade-off："全局聚合查询要打所有 Shard，用缓存+后台预计算解决"
5. 说明扩展计划："从64个 Shard 起步，Consistent Hashing 保证扩容时数据移动最少"
```

------

**一句话记忆**

> **Sharding = 数据水平切割到多台机器；首选 Hash-Based；核心挑战是热点、跨片查询、跨片事务；设计 Shard Key 时要让常见查询只打一台机器。**

## Consistent Hashing

普通取模哈希：`shard = hash(key) % N`

**致命缺陷：N 变化时，几乎所有数据都要重新分配**

> 从3个节点加到4个节点 → `% 3` 变 `% 4` → 80%+ 的数据需要迁移 → 系统崩溃

------

**Consistent Hashing 核心思想**

把**数据**和**节点**都放在同一个"哈希环"上（范围 0 ~ 2³²）

**路由规则：** 数据落点 → 顺时针找最近的节点

```
          DB1(0)
       /          \
  DB4(75)        DB2(25)
       \          /
          DB3(50)

event #1234 → hash值落在10 → 顺时针 → DB2
```

------

**加/减节点时的优势**

**加节点（DB5 放在90处）：**

- 只有落在 75~90 之间的数据需要从 DB1 迁移到 DB5
- 其余数据完全不动 ✅

**减节点（DB2 宕机）：**

- 只有原来在 DB2 上的数据迁移到 DB3
- 其余数据完全不动 ✅

> **结论：只影响相邻节点，数据移动量从"全部"降到"1/N"**

------

**Virtual Nodes（虚拟节点）解决什么？**

**问题：** DB2 宕机后，所有数据涌向 DB3，DB3 压力翻倍

**解决：** 每个节点在环上占多个位置

```
DB1 → hash("DB1-vn1"), hash("DB1-vn2"), hash("DB1-vn3")...
DB2 → hash("DB2-vn1"), hash("DB2-vn2"), hash("DB2-vn3")...
```

这样各节点的虚拟节点**交错分布**在环上，DB2 宕机后：

- DB2-vn1 的数据 → 给 DB1
- DB2-vn2 的数据 → 给 DB3
- DB2-vn3 的数据 → 给 DB4

**负载均匀分散到所有节点，而不是压垮一个邻居** ✅

------

**热点问题（Hot Spot）**

Consistent Hashing 保证 key 分布均匀，**但不保证流量均匀**

> Taylor Swift 演唱会门票：同一个 key，但请求量是其他的100倍

| 方案         | 做法                                               |
| ------------ | -------------------------------------------------- |
| Read Replica | 热点数据复制多份，读请求负载均衡                   |
| Key Salting  | `taylor-swift-0` ~ `taylor-swift-9` 分散到不同节点 |

**记忆口诀：**

- Virtual Nodes → 解决**key分布不均**（结构问题）
- Replication/Salting → 解决**流量不均**（热点问题）

------

**真实系统应用**

| 系统          | 使用方式                                 |
| ------------- | ---------------------------------------- |
| Cassandra     | 标准 Consistent Hashing + Virtual Nodes  |
| DynamoDB      | 内部使用，自动处理热点迁移               |
| CDN           | 决定内容缓存在哪个边缘节点               |
| Redis Cluster | **不用**，用固定 16384 个 Hash Slot 代替 |

------

**面试答题时机**

**大多数面试：** 说"DynamoDB/Cassandra 内部用 Consistent Hashing 处理分片" 就够了

**基础设施深度面试（设计分布式DB/Cache/MQ）：** 需要完整解释：

1. 为什么取模哈希不行
2. 哈希环如何工作
3. Virtual Nodes 如何均衡负载
4. 如何处理热点

------

**一句话记忆**

> **Consistent Hashing = 哈希环 + 顺时针找节点；加减节点只影响相邻数据；Virtual Nodes 让负载均匀分散；解决的是"扩容时数据大规模迁移"的问题。**

## CAP Theorem

> ## 一、CAP 是什么？
>
> 分布式系统中，以下三个特性**最多只能同时满足两个**：
>
> | 字母  | 特性                            | 含义                                       |
> | ----- | ------------------------------- | ------------------------------------------ |
> | **C** | Consistency（一致性）           | 所有节点在同一时刻看到相同数据             |
> | **A** | Availability（可用性）          | 每个请求都能得到响应（数据可能不是最新的） |
> | **P** | Partition Tolerance（分区容错） | 网络断了，系统仍然继续运行                 |
>
> **Partition tolerance** refers to the functioning of a cluster even if there is a "partition" (communication break) between two nodes (both nodes are up, but can't communicate).
>
> ------
>
> ## 二、关键简化：P 是必选项
>
> > 网络故障在分布式系统中**不可避免**，所以 P 必须保留。
>
> 因此 CAP 实际上只剩**一个选择**：
>
> ```
> 网络分区发生时，你优先保证 C 还是 A？
> ```
>
> ------
>
> ## 三、怎么选？一个问题判断
>
> > **"如果用户短暂看到不一致的数据，会是灾难性的吗？"**
>
> - Option A: Return an error because we can't guarantee the data is up-to-date (choosing consistency)
> - Option B: Show potentially stale data (choosing availability)
>
> | 是 → 选 **C（一致性）**  | 否 → 选 **A（可用性）**  |
> | ------------------------ | ------------------------ |
> | 机票/演唱会座位预订      | 社交媒体（头像、动态）   |
> | 电商库存（最后一件商品） | 视频平台（电影简介）     |
> | 金融交易、股票报价       | 点评网站（餐厅营业时间） |
> | 银行账户余额             | Feed流、点赞数           |
>
> ------
>
> ## 四、选不同方向时的设计差异
>
> ### 选 C（一致性）→ 系统更复杂，延迟更高
>
> - 用**单一数据库节点**（单点真实来源）
> - 用**分布式事务**（两阶段提交）保证多节点同步
> - 技术选型：PostgreSQL、MySQL、Google Spanner
>
> ### 选 A（可用性）→ 系统扩展性好，允许最终一致
>
> - 用**多个读副本**（异步复制，副本可能稍落后）
> - 用 **CDC**（Change Data Capture）异步传播数据变更
> - 技术选型：Cassandra、Redis Cluster、DynamoDB（多可用区配置）
>
> ------
>
> ## 五、进阶：同一系统不同功能选不同策略
>
> 真实系统往往**不是非此即彼**，不同功能需要不同策略：
>
> **Ticketmaster 例子：**
>
> - 预订座位 → 选 **C**（防止双重预订）
> - 浏览活动详情 → 选 **A**（活动描述稍旧无所谓）
>
> **Tinder 例子：**
>
> - 匹配（双方同时右滑）→ 选 **C**（必须立即看到匹配）
> - 查看用户主页 → 选 **A**（头像稍旧无所谓）
>
> > 💡 面试中说出这种分层思考会很加分！
>
> ------
>
> ## 六、一致性强度的四个级别（进阶了解）
>
> 从强到弱：
>
> ```
> 强一致性    → 读到的一定是最新写入（银行账户）
> 因果一致性  → 有因果关系的事件顺序一致（评论一定在帖子之后）
> 读己写一致  → 自己的修改自己立刻看到，别人可能稍晚看到（社交媒体）
> 最终一致性  → 最终会一致，但中间可能有延迟（DNS、大多数分布式数据库默认行为）
> ```
>
> ------
>
> ## 七、面试中何时提 CAP？
>
> **在讨论非功能性需求时，第一个提！**
>
> ```
> 面试流程：
> 1. 功能性需求（做什么）
> 2. 非功能性需求 ← 这里先问 CAP
> 
> 面试金句：
> "在讨论非功能性需求之前，我想先确认一个关键问题：
> 这个系统是否需要强一致性？
> 比如[具体场景]，如果出现数据不一致会不会是灾难性的？"
> ```
>
> ------
>
> ## 八、一句话记忆法
>
> > **P必选 → 只剩C vs A → 问"数据不一致是灾难吗" → 是就选C，否就选A → 同一系统不同功能可以分别选**

## Database Indexing

> ## 一、索引是什么？（先建直觉）
>
> 想象一本1000页的字典，查"苹果"这个词：
>
> - **没有索引**：从第1页翻到第1000页（全表扫描，O(n)）
> - **有索引**：直接翻到"A"区域（O(log n) 或 O(1)）
>
> 索引的本质：**用额外的数据结构换取查询速度**，代价是写入变慢、占用更多存储。
>
> ------
>
> ## 二、五种索引类型
>
> ### 1. B-Tree Index（最重要，面试默认）
>
> **数据结构**：平衡多叉树
>
> ```
>                   [30 | 70]
>                  /    |    \
>           [10|20]  [40|50]  [80|90]
>           / | \    / | \    / | \
>          叶节点（存储实际数据行的指针）
> ```
>
> **关键特性**：
>
> - 叶节点按顺序排列，且通过**双向链表**相连
> - 支持**等值查询**：`WHERE age = 25`
> - 支持**范围查询**：`WHERE age BETWEEN 20 AND 30`
> - 支持**排序**：`ORDER BY age`（因为叶节点本来就有序）
> - 查找复杂度：**O(log n)**
>
> **类比**：图书馆书架按编号排列，你既可以找特定编号，也可以找"100-200号之间的书"。
>
> **使用场景**：几乎所有数据库的**默认索引**类型。PostgreSQL、MySQL 的索引默认就是 B-Tree。
>
> ------
>
> ### 2. LSM Tree（Log-Structured Merge Tree）
>
> **核心思想**：把随机写变成顺序写，极大提升写入性能。
>
> ------
>
> ## 一、为什么需要 LSM Tree？先理解问题
>
> 硬盘有两种写入方式：
>
> | 写入方式   | 速度           | 原因                   |
> | ---------- | -------------- | ---------------------- |
> | **顺序写** | 快（~500MB/s） | 磁头不需要移动，连续写 |
> | **随机写** | 慢（~100MB/s） | 磁头反复跳来跳去       |
>
> B-Tree 的问题：更新数据时，需要找到树中对应节点修改 → **随机写**，磁盘慢。
>
> LSM Tree 的解法：**永远只做顺序写**，把随机写的问题推迟到后台处理。
>
> ------
>
> ## 二、LSM Tree 完整架构
>
> ```
> 写入请求
>     ↓
>  WAL（预写日志，磁盘顺序追加）← 防止崩溃丢数据
>     ↓
>  MemTable（内存，红黑树/跳表，有序）
>     ↓ 满了（通常几MB）
>  SSTable L0（磁盘，不可变，有序）
>     ↓ L0文件太多
>  SSTable L1
>     ↓ Compaction
>  SSTable L2 → L3 → ...（越深越大）
> ```
>
> ------
>
> ## 三、逐步拆解每个组件
>
> ### WAL（Write Ahead Log，预写日志）
>
> 写数据时**同时写** `WAL 和 MemTable。`
>
> WAL 的唯一作用：**防止崩溃丢数据**。
>
> > 如果机器在 MemTable 还没刷到磁盘时突然断电 → MemTable 内容消失 → 但 WAL 还在磁盘上 → 重启时重放 WAL 恢复数据。
>
> WAL 本身是顺序追加写，极快。
>
> ------
>
> ### MemTable（内存有序结构）
>
> - 用**红黑树**或**跳表**实现，保持 key 有序
> - 所有写入先到这里：新增、修改、删除都写这里
> - 读取时**先查 MemTable**（最新数据在这）
> - 满了之后，整体 flush 到磁盘变成一个 SSTable
>
> > 💡 为什么要保持有序？因为 flush 到磁盘时直接顺序写出有序的 SSTable，无需额外排序。
>
> ------
>
> ### SSTable（Sorted String Table，有序字符串表）
>
> 写入磁盘后**永不修改**（Immutable），只能新建或删除整个文件。
>
> 每个 SSTable 内部的数据按 key 排序：
>
> ```
> SSTable 文件内部结构：
> ┌─────────────────────────────────┐
> │ apple → 10                      │
> │ banana → 25                     │ ← Segment（每个文件由多个段组成）
> │ cat → 7                         │
> ├─────────────────────────────────┤
> │ dog → 52                        │
> │ elephant → 3                    │
> └─────────────────────────────────┘
> ```
>
> 同一个 key 可能出现在**多个 SSTable** 里（不同时间写入的不同版本），越新的 SSTable 里的值越新。
>
> ------
>
> ### 删除：墓碑机制（Tombstone）
>
> LSM Tree **不能直接删除数据**（文件不可变）。
>
> 删除 = 写入一条特殊标记 tombstone：
>
> ```
> dog → [TOMBSTONE]   ← 这条记录表示"dog已被删除"
> ```
>
> 读取时碰到 tombstone → 返回"不存在"。
>
> Compaction 时才真正清除 tombstone 和旧数据。
>
> > 💡 结论：LSM Tree 的删除操作**先占用空间**，等 Compaction 后才释放。
>
> ------
>
> ## 四、读取流程（最复杂的部分）
>
> 查询 key "dog"：
>
> ```
> 1. 查 MemTable（内存，最新）
>       ↓ 没找到
> 2. 查 L0 SSTable（从最新的文件开始）
>       ↓ 没找到
> 3. 查 L1 SSTable
>       ↓ 没找到
> 4. 查 L2 ... 直到找到
> ```
>
> **最坏情况要翻遍所有层** → 读取慢是 LSM Tree 的核心缺点。
>
> ### 两大优化手段
>
> **① 稀疏索引（Sparse Index）**
>
> 不对每个 key 建索引，只记录每个 Segment 开头和结尾的 key：
>
> ```
> 稀疏索引：
> apple  → offset 0
> cat    → offset 1024
> dog    → offset 2048
> ```
>
> 查 "banana" → 二分搜索发现在 apple 和 cat 之间 → 只扫描 0~1024 这段，不用扫全文件。
>
> **② Bloom Filter（布隆过滤器）**
>
> 每个 SSTable 配一个 Bloom Filter。
>
> 查询前先问 Bloom Filter："dog 在这个文件里吗？"
>
> - 回答"不在" → **100% 不在**，跳过这个文件
> - 回答"在" → **可能在**（有极小概率误报），再去查
>
> **效果**：查一个不存在的 key，几乎不需要读任何 SSTable 文件，直接被 Bloom Filter 拦截。
>
> ------
>
> ## 五、Compaction（后台合并）
>
> 随着时间推移，SSTable 文件越积越多，两个问题：
>
> 1. 文件太多，读取要查很多层
> 2. 同一个 key 存在多份旧版本，浪费空间
>
> Compaction = 后台线程定期把多个 SSTable **归并排序**成一个大文件，同时：
>
> - 丢弃同一 key 的旧版本，只保留最新
> - 清除 tombstone 标记的删除数据
>
> ```
> Compaction 前：
> SSTable1: dog→52, elephant→3
> SSTable2: dog→[TOMBSTONE], fish→8
>               ↓ Compaction
> SSTable3: elephant→3, fish→8
> （dog的旧值和tombstone都被清除）
> ```
>
> **Compaction 是 LSM Tree 的主要性能开销**，会占用 CPU 和磁盘 I/O，但它在后台运行，不影响前台写入。
>
> ------
>
> ## 六、完整读写流程总结
>
> ```
> 写入：WAL（顺序写）→ MemTable（内存）→ 满了flush→ SSTable（磁盘）
>                                               ↑ 后台Compaction合并
> 
> 读取：MemTable → L0 SSTables → L1 → L2...
>       （Bloom Filter 快速跳过不含该key的文件）
>       （稀疏索引 精确定位文件内偏移量）
> 
> 删除：写 tombstone → Compaction 时真正清除
> ```
>
> ------
>
> ## 七、B-Tree vs LSM Tree 对比
>
> | 对比维度       | B-Tree            | LSM Tree                 |
> | -------------- | ----------------- | ------------------------ |
> | **写入速度**   | 中（随机写磁盘）  | **极快**（顺序写内存）   |
> | **读取速度**   | **快**            | 中（可能查多层）         |
> | **空间放大**   | 低                | 高（多版本+tombstone）   |
> | **写入放大**   | 中                | 高（Compaction反复写）   |
> | **适合场景**   | 读多写少，OLTP    | **写多读少**，日志，时序 |
> | **代表数据库** | PostgreSQL, MySQL | Cassandra, RocksDB       |
>
> ------
>
> ## 八、面试金句
>
> > **"如果系统是写密集型的，比如用户行为日志、时序数据，我会考虑使用基于 LSM Tree 的数据库如 Cassandra。LSM Tree 通过将所有写入先缓冲在内存的 MemTable 中，然后顺序 flush 到磁盘，把随机写变成顺序写，写入性能极高。读取时可能需要查多个层级，但通过 Bloom Filter 和稀疏索引可以大幅优化。"**
>
> ------
>
> ### 3. Hash Index
>
> **数据结构**：哈希表
>
> ```
> key → hash函数 → bucket → 数据位置
> "user:123" → hash() → bucket[42] → 指向行数据
> ```
>
> **特性**：
>
> - 等值查询极快：**O(1)**
> - **不支持范围查询**（哈希后顺序被打散）
> - **不支持排序**
>
> **使用场景**：只需要精确匹配的场景
>
> - Redis 的默认存储结构
> - `WHERE user_id = 123`（不需要范围查询）
> - 数据库连接操作中的 Hash Join
>
> > ⚠️ 面试陷阱：Hash Index 不支持 `BETWEEN`、`>`、`<`、`ORDER BY`，B-Tree 都支持。
>
> ------
>
> ### 4. Geospatial Index（地理空间索引）
>
> **问题**：如何快速找到"距离我3km内的餐厅"？普通B-Tree无法处理二维坐标。
>
> ### 1. **网格索引**
>
> **网格索引**是组织地理空间数据最简单的方法之一。可以将城市地图分成若干个方块，就像棋盘一样。每个方块覆盖特定的区域，区域内的地点（如餐馆或商店）被归类在一起。
>
> ![img](C:\Learning Notes\Java\系统设计之神\img\v2-b8a0192b85efc48afd796db8eb3e8b6d_1440w.jpg)
>
> 网格索引
>
> **网格索引的工作原理：**
>
> 1. **将地图划分**为相同大小的网格方块。
> 2. **根据坐标**（纬度和经度）将地点分配到网格方块中。
> 3. **查找附近地点**时，只需检查你所在的方块以及周围的几个方块。
>
> 例子：
>
> 假设你有一个小镇的地图，并将其划分为每边1公里的方格。以下是一个示例网格布局：
>
> | 网格ID | 左上角坐标 (纬度, 经度) | 右下角坐标 (纬度, 经度) |
> | ------ | ----------------------- | ----------------------- |
> | A1     | (37.8, -122.4)          | (37.7, -122.3)          |
> | A2     | (37.8, -122.3)          | (37.7, -122.2)          |
> | B1     | (37.7, -122.4)          | (37.6, -122.3)          |
> | B2     | (37.7, -122.3)          | (37.6, -122.2)          |
>
> 假设你位于位置 (37.75, -122.35)，你在**B1**网格中。要查找附近的餐馆，我们只需要检查**B1**以及相邻的网格（如**A1**, **A2**, **B2**），而不必搜索整个地图。
>
> ------
>
> ### 2. **[R-Tree](https://zhida.zhihu.com/search?content_id=248214666&content_type=Article&match_order=1&q=R-Tree&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NzE5MDU5NjYsInEiOiJSLVRyZWUiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoyNDgyMTQ2NjYsImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.rmEy4agp-EaXfRA9TCtSl-XRczxBCC6vYgoXfQScvV0&zhida_source=entity)（矩形树）**
>
> **R-Tree**比网格索引更高级。它不是将地图划分为相同大小的方块，而是创建包围数据点的矩形。每个矩形可以包含其他矩形，形成一个层次结构。
>
> ![img](C:\Learning Notes\Java\系统设计之神\img\v2-1baef357a8d71ba2b75195ba162384d4_1440w.jpg)
>
> 矩形树 - 世界地图划分
>
> **R-Tree的工作原理：**
>
> 1. **创建包围矩形**，将附近的地点分组。
> 2. **将较小的矩形**分组为较大的矩形，形成层次结构。
> 3. 搜索时，**逐步缩小范围**，检查哪个矩形包含你的位置，直到找到具体的地点。
>
> ![img](C:\Learning Notes\Java\系统设计之神\img\v2-f87167bda3ec9ec0df0ace066da025e5_1440w.jpg)
>
> 矩形树-数据结构
>
> 例子：
>
> 假设在一个街区内有四家餐馆：
>
> - 餐馆1： (37.76, -122.43)
> - 餐馆2： (37.75, -122.42)
> - 餐馆3： (37.74, -122.41)
> - 餐馆4： (37.73, -122.40)
>
> 我们可以将**餐馆1**和**餐馆2**分组为一个小矩形，将**餐馆3**和**餐馆4**分组为另一个矩形。然后，再创建一个更大的矩形覆盖这两个小组。在搜索时，算法先检查大的矩形，再缩小到较小的矩形，最终找到具体的餐馆。
>
> ------
>
> ### 3. **[Geohash](https://zhida.zhihu.com/search?content_id=248214666&content_type=Article&match_order=1&q=Geohash&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NzE5MDU5NjYsInEiOiJHZW9oYXNoIiwiemhpZGFfc291cmNlIjoiZW50aXR5IiwiY29udGVudF9pZCI6MjQ4MjE0NjY2LCJjb250ZW50X3R5cGUiOiJBcnRpY2xlIiwibWF0Y2hfb3JkZXIiOjEsInpkX3Rva2VuIjpudWxsfQ.VHBxVWdX7rhF8TUzPY2DuCOLe4_g3OsXLKC4cBu0VLo&zhida_source=entity)**
>
> **Geohash**是一种将纬度和经度编码为字母和数字字符串的算法。附近的地点拥有相似的geohash前缀，这使得搜索附近地点变得容易。
>
> ![img](C:\Learning Notes\Java\系统设计之神\img\v2-7a669525e96e11e9fb19ec4c21ee7c17_1440w.jpg)
>
> 去这个网站试试：https://www.movable-type.co.uk/scripts/geohash.html
>
> **Geohash的工作原理：**
>
> 1. **将地球划分**为一个网格（网格的大小可以不同）。
> 2. **为每个网格单元**分配一个唯一的代码（hash）。
> 3. **细分网格**，为较小的区域创建更精确的代码。
> 4. 搜索时，只需要比较具有相似geohash前缀的地点。
>
> 例子：
>
> 假设你有以下几个位置：
>
> - 位置1 (37.77, -122.43) 的 geohash 是 `9q8yy`。
> - 位置2 (37.75, -122.42) 的 geohash 是 `9q8yx`。
> - 位置3 (37.74, -122.41) 的 geohash 是 `9q8yv`。
>
> 如果你位于 (37.76, -122.43)，你的 geohash 可能是 `9q8yy`，你只需查看前缀相同的地点（例如`9q8y`开头的地方），这大大缩小了搜索范围。
>
> ------
>
> ### 5. Inverted Index（倒排索引）
>
> **问题**：如何实现全文搜索？比如搜索"系统设计"能找到包含这两个词的所有文章？
>
> **正向索引** vs **倒排索引**：
>
> ```
> 正向：文档ID → 包含哪些词
> 文档1 → ["系统", "设计", "面试"]
> 文档2 → ["系统", "架构", "数据库"]
> 
> 倒排：词 → 出现在哪些文档
> "系统" → [文档1, 文档2]
> "设计" → [文档1]
> "架构" → [文档2]
> ```
>
> 搜索"系统 AND 设计" → 取"系统"的列表和"设计"的列表求**交集** → 文档1
>
> **附加信息**：倒排索引还会存储词频（TF）、词在文档中的位置，用于**相关性排序**。
>
> **使用场景**：搜索引擎、Elasticsearch、全文搜索功能 代表技术：**Elasticsearch、Apache Lucene**
>
> ------
>
> ## 三、索引优化模式
>
> ### Composite Index（复合索引）
>
> 把多个列组合成一个索引。
>
> **关键规则：最左前缀原则**
>
> ```sql
> -- 创建复合索引 (user_id, created_at, status)
> INDEX ON posts(user_id, created_at, status)
> 
> -- ✅ 可以用到索引
> WHERE user_id = 1
> WHERE user_id = 1 AND created_at > '2024-01-01'
> WHERE user_id = 1 AND created_at > '2024-01-01' AND status = 'published'
> 
> -- ❌ 用不到索引（跳过了最左列）
> WHERE created_at > '2024-01-01'
> WHERE status = 'published'
> ```
>
> **类比**：电话簿按"姓 + 名"排序。你可以查"张某某"，也可以只查"张"，但不能只查"某某"（因为名字没有单独排序）。
>
> **列顺序选择原则**：
>
> 1. **等值查询的列放前面**（WHERE user_id = 1）
> 2. **范围查询的列放最后**（WHERE created_at > X）
> 3. **高区分度的列放前面**（gender 只有2个值，区分度低，放后面）
>
> ------
>
> ### Covering Index（覆盖索引）
>
> **核心思想**：查询所需的所有列都在索引里，**不需要回表查原始数据**。
>
> > Covering index 就是：
> >  查询所需要的所有字段，都已经**存在于某个二级索引的叶子节点中**，因此不需要回表。
>
> **普通查询流程**：
>
> ```
> 查索引 → 找到行的物理位置 → 回表查原始数据 → 返回结果
> （两次I/O）
> ```
>
> **覆盖索引查询流程**：
>
> ```
> 查索引 → 索引本身包含所需数据 → 直接返回
> （一次I/O）
> ```
>
> **例子**：
>
> ```sql
> -- 查询：某用户的所有帖子标题
> SELECT title FROM posts WHERE user_id = 1
> 
> -- 普通索引只有 user_id：找到行位置后还要回表拿title
> INDEX ON posts(user_id)
> 
> -- 覆盖索引：索引直接包含title
> INDEX ON posts(user_id, title)  ← 查询时完全不需要读原表
> ```
>
> > 💡 面试金句：**"为了避免回表，我会把查询中SELECT的列也加入复合索引，形成覆盖索引。"**
>
> ------
>
> ## 四、面试实战：如何讨论索引
>
> ### 场景例题：设计 Twitter，如何设计 posts 表的索引？
>
> **先看 API 需要什么查询：**
>
> ```
> GET /feed         → 查某用户关注的人的最新帖子
> GET /user/{id}/posts  → 查某用户的所有帖子，按时间排序
> GET /search       → 全文搜索帖子内容
> ```
>
> **对应索引设计：**
>
> ```sql
> -- 查某用户的帖子，按时间排序
> INDEX ON posts(user_id, created_at DESC)
> -- 复合索引，user_id等值查询在前，时间范围在后
> 
> -- 查 feed（关注的人的帖子）
> INDEX ON posts(user_id, created_at DESC)
> -- 同一个索引复用，对每个关注的人查询后合并
> 
> -- 全文搜索内容
> -- 用 Elasticsearch（倒排索引），不用数据库索引
> ```
>
> **如果面试官问"为什么这样排列列顺序"：**
>
> > "user_id 是等值查询，区分度高（每个用户不同），放在前面。created_at 是范围查询，放在后面。这样符合最左前缀原则，等值条件能充分缩小范围后再做时间过滤。"
>
> ------
>
> ## 五、各种索引对比总结
>
> | 索引类型       | 查询类型       | 写入性能 | 代表技术       | 面试场景       |
> | -------------- | -------------- | -------- | -------------- | -------------- |
> | **B-Tree**     | 等值+范围+排序 | 中       | PostgreSQL默认 | **默认选这个** |
> | **LSM Tree**   | 等值（读慢）   | 极快     | Cassandra      | 海量写入系统   |
> | **Hash**       | 仅等值 O(1)    | 快       | Redis          | 缓存、精确匹配 |
> | **Geospatial** | 空间范围       | 中       | PostGIS, Redis | 附近X功能      |
> | **Inverted**   | 全文搜索       | 慢       | Elasticsearch  | 搜索功能       |
>
> ------
>
> ## 六、索引的代价（必须会说）
>
> > 面试中提出索引之后，一定要同时说代价，展示你理解tradeoff。
>
> - **写入变慢**：每次写入/更新，都要同时维护索引数据结构
> - **占用存储**：索引本身需要额外磁盘空间
> - **索引过多反而有害**：写多读少的表上加太多索引，写入性能会崩
>
> ------
>
> ## 七、一句话记忆法
>
> > **B-Tree是默认；LSM写入猛；Hash只等值；地理用R-Tree/Geohash；全文用倒排。复合索引记"等值在前+范围在后+最左前缀"；覆盖索引把SELECT的列放进去避免回表。**

## Numbers to Know

# Pattern

